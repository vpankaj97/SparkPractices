{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\pyspark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#For Local Machine\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession. \\\n",
    "builder. \\\n",
    "appName('eXAMPLES'). \\\n",
    "master('local'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-249MU1B:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>eXAMPLES</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x6ba4b00>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "[u'Think of it for a moment \\u2013 1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting,  it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.', u'', u'Big Data is not a new phenomena. It has been around for a while now. However, it has become really important with this pace of data generation. In past, several systems were developed for processing big data. Most of them were based on MapReduce framework. These frameworks typically rely on use of hard disk for saving and retrieving the results. However, this turns out to be very costly in terms of time and speed.', u'', u'On the other hand, Organizations have never been more hungrier to add a competitive differentiation through understanding this data and offering its customer a much better experience. Imagine how valuable would be Facebook, if it did not understand your interests well? The traditional hard disk based MapReduce kind of frameworks do not help much to address this challenge.']\n",
      "[u'Think', u'of', u'it']\n",
      "[(u'Think', 1), (u'of', 1), (u'it', 1), (u'for', 1), (u'a', 1)]\n",
      "[(u'', 274), (u'diff_cat_in_train_test.distinct().count()#', 1), (u'better.', 1)]\n",
      ": 274\n",
      "diff_cat_in_train_test.distinct().count()#: 1\n",
      "better.: 1\n",
      "saved: 1\n",
      "(which: 1\n",
      "cluster).: 1\n",
      "Scala,: 1\n",
      "Scala.: 1\n",
      "sc.parallelize(data): 2\n",
      "sc.parallelize(data,: 1\n",
      "Apache: 52\n",
      "Scala:: 1\n",
      "saves: 2\n",
      "connects: 1\n",
      "Support:: 1\n",
      "skills: 1\n",
      "solution: 3\n",
      "Occupation: 1\n",
      "created.: 1\n",
      "elegant: 1\n",
      "second: 1\n",
      "machines: 5\n",
      "even: 1\n",
      "‘all”: 1\n",
      "ppa:webupd8team/java: 1\n",
      "terms,: 2\n",
      "selected: 1\n",
      "pyspark.ml.evaluation.: 1\n",
      "above: 2\n",
      "new: 7\n",
      "increasing: 1\n",
      "np.sqrt(mse),: 1\n",
      "metadata: 1\n",
      "(Spark: 1\n",
      "machine.: 1\n",
      "machine): 1\n",
      "never: 1\n",
      "path: 3\n",
      "100: 2\n",
      "(Directed: 1\n",
      "apt-add-repository: 1\n",
      "error.: 1\n",
      "divide: 3\n",
      "Support: 1\n",
      "replace: 1\n",
      "bashrc: 3\n",
      "~/Downloads: 2\n",
      "would: 7\n",
      "concept.: 1\n",
      "call: 5\n",
      "type: 4\n",
      "until: 2\n",
      "separated: 1\n",
      "count,: 1\n",
      "brings: 1\n",
      "must: 1\n",
      "me: 1\n",
      "Foundation.: 1\n",
      "ml: 1\n",
      "work: 3\n",
      "Header: 1\n",
      "mv: 1\n",
      "install: 9\n",
      "root: 3\n",
      "example: 1\n",
      "./bin/run-example: 1\n",
      "give: 2\n",
      "Virtual: 1\n",
      "want: 8\n",
      "type:: 1\n",
      "keep: 1\n",
      "end: 1\n",
      "provide: 2\n",
      "algorithm.: 1\n",
      "feature: 1\n",
      "machine: 4\n",
      "how: 9\n",
      "Rdd.map(lambda: 1\n",
      "A: 2\n",
      "transforming: 1\n",
      "after: 5\n",
      "test_cv: 2\n",
      "fillna: 1\n",
      "parallel: 4\n",
      "types: 4\n",
      "All: 2\n",
      "train.fillna(-1): 1\n",
      "perform: 1\n",
      "algorithms: 1\n",
      "sys.path:: 1\n",
      "Another: 1\n",
      "things: 2\n",
      "first: 16\n",
      "GB: 1\n",
      "operations: 10\n",
      "over: 5\n",
      "digest: 1\n",
      "before: 2\n",
      "(more: 1\n",
      ",: 2\n",
      "Here: 3\n",
      "writing: 3\n",
      "better: 2\n",
      "(train_cv,: 1\n",
      "then: 3\n",
      "them: 8\n",
      "There: 6\n",
      "combination: 1\n",
      "operation:: 1\n",
      "break: 1\n",
      "they: 2\n",
      "operation.: 4\n",
      "list,: 1\n",
      "http://d3kbcqa49mib13.cloudfront.net/spark-1.6.0.tgz: 1\n",
      "classified: 1\n",
      "API: 3\n",
      "each: 14\n",
      "side: 2\n",
      "mean: 2\n",
      "covered: 1\n",
      "upload: 1\n",
      "(14.04): 1\n",
      "extract: 2\n",
      "speed.: 3\n",
      "overcame: 1\n",
      "network: 1\n",
      "finished,: 1\n",
      "~/Downloads/spark-1.6.0: 2\n",
      "content: 1\n",
      "rf: 2\n",
      "got: 1\n",
      "is!: 1\n",
      "Spark:: 1\n",
      "Spark?: 3\n",
      "University: 1\n",
      "free: 1\n",
      "Spark.: 12\n",
      "Spark,: 3\n",
      "Spark!: 1\n",
      "created: 5\n",
      "messages: 1\n",
      "tolerant: 1\n",
      "creates: 1\n",
      "Computation: 1\n",
      "already: 9\n",
      "features: 7\n",
      "Please: 2\n",
      "another: 1\n",
      "Train1,Test1: 1\n",
      "APIs: 1\n",
      "top: 1\n",
      "P.S.: 1\n",
      "master: 1\n",
      "observations: 2\n",
      "test.fillna(-1): 1\n",
      "('Ankit: 1\n",
      "tool: 1\n",
      "Deserialization.: 1\n",
      "10)),: 1\n",
      "feature.: 2\n",
      "Test1: 1\n",
      "Set: 1\n",
      "Features: 1\n",
      "Previewing: 1\n",
      "professional.: 1\n",
      "tree: 1\n",
      "train1.select('features').show(): 1\n",
      "API.: 2\n",
      "enables: 2\n",
      "Java’s: 1\n",
      "well?: 1\n",
      "‘log4j.properties’,: 1\n",
      "well:: 1\n",
      "SPARK_HOME: 1\n",
      "pass: 2\n",
      "mind: 1\n",
      "Database): 1\n",
      "data”: 1\n",
      "talking: 1\n",
      "manner: 1\n",
      "installation,: 1\n",
      "tells: 1\n",
      "data,: 2\n",
      "data.: 6\n",
      "data): 1\n",
      "Java,: 2\n",
      "map-reduce: 3\n",
      "fitting: 1\n",
      "installed,: 1\n",
      "do?: 1\n",
      "They: 1\n",
      "data:: 1\n",
      "Softwares: 1\n",
      "though: 2\n",
      "object: 1\n",
      "metrics: 1\n",
      "Shell,: 1\n",
      "process).: 1\n",
      "‘select’: 1\n",
      "SQLContext: 1\n",
      "(Hard: 1\n",
      "momentum.: 1\n",
      "labels: 2\n",
      "Hadoop: 9\n",
      "main(): 1\n",
      "solutions: 1\n",
      "'prediction: 1\n",
      "shuffling: 2\n",
      "Rdd1.collect(): 1\n",
      "Big: 8\n",
      "explain: 1\n",
      "Distributed: 4\n",
      "do: 11\n",
      "sqlContext.load(source=\"com.databricks.spark.csv\",: 2\n",
      "null.: 1\n",
      "HDFS,: 1\n",
      "solution.: 2\n",
      "Accumulator: 2\n",
      "competitive: 1\n",
      "report: 1\n",
      "them?: 1\n",
      "RegressionEvaluator(): 1\n",
      "labeller.transform(train): 1\n",
      "bad: 1\n",
      "./bin/pyspark: 3\n",
      "them.: 2\n",
      "them): 1\n",
      "fascinating: 1\n",
      "machine,: 1\n",
      "result: 4\n",
      "techniques: 1\n",
      "Cluster: 2\n",
      "0:: 1\n",
      "monitoring.: 1\n",
      "+City_Category+Stay_In_Current_City_Years+Product_Category_1+Product_Category_2+: 1\n",
      "score: 1\n",
      "xvf: 1\n",
      "lazy: 3\n",
      "terms: 5\n",
      "Hadoop.: 3\n",
      "Hadoop,: 1\n",
      "diff_cat_in_train_test=test.select('Product_ID').subtract(train.select('Product_ID')): 1\n",
      "extent: 1\n",
      "Here,: 1\n",
      "fault: 1\n",
      "train.select('User_ID').show(): 1\n",
      "method.We: 1\n",
      "directly.: 1\n",
      "'/opt/spark': 1\n",
      "SparkContext: 9\n",
      "Given: 1\n",
      "tough: 1\n",
      "Product_ID,: 1\n",
      "Forest: 1\n",
      "iterable: 1\n",
      "now,: 4\n",
      "now.: 2\n",
      "“In-memory: 1\n",
      "three: 2\n",
      "been: 2\n",
      ".: 2\n",
      "much: 8\n",
      "tutorials: 1\n",
      "transformed: 3\n",
      "Billion!: 1\n",
      "computation“,: 1\n",
      "deeper: 1\n",
      "(to: 1\n",
      "worker: 2\n",
      "Supports: 1\n",
      "Spark),: 1\n",
      "applied: 2\n",
      "train_cv: 3\n",
      "sparkContext(): 1\n",
      "tutorial.: 1\n",
      "test1.: 2\n",
      "Train1.show(): 1\n",
      "n: 2\n",
      "True,inferSchema: 2\n",
      "launched: 1\n",
      "played: 1\n",
      "is: 93\n",
      "it: 27\n",
      "2009.: 1\n",
      "in: 127\n",
      "if: 9\n",
      "Python: 13\n",
      "suggest: 1\n",
      "make: 1\n",
      "3773.1460883883865: 1\n",
      "program:: 1\n",
      "complex: 1\n",
      "big: 7\n",
      "program.: 5\n",
      "several: 8\n",
      "log4j: 1\n",
      "independent: 2\n",
      "numerical: 4\n",
      "categories:: 1\n",
      "Dataframe: 14\n",
      "(rows): 1\n",
      "Table: 1\n",
      "the: 186\n",
      "RDDs: 2\n",
      "just: 5\n",
      "PySpark: 4\n",
      "(Transformation): 1\n",
      "distribute: 1\n",
      "previous: 2\n",
      "adding: 2\n",
      "Lazy: 2\n",
      "processes,: 1\n",
      "had: 1\n",
      "Framework: 1\n",
      "Imputing: 1\n",
      "failures.: 1\n",
      "save: 4\n",
      "Rdd1.: 1\n",
      "Building: 1\n",
      "RDD:: 2\n",
      "unique: 1\n",
      "RDD.: 8\n",
      "RDD,: 6\n",
      "5:: 1\n",
      "55: 1\n",
      "performing: 2\n",
      "steps: 2\n",
      "security: 1\n",
      "Now,: 2\n",
      "Let’s: 11\n",
      "right: 1\n",
      "located.: 1\n",
      "deal: 1\n",
      "people: 1\n",
      "successfully: 2\n",
      "for: 53\n",
      "bottom: 1\n",
      "/: 10\n",
      "Let: 7\n",
      "External: 1\n",
      "3822.121053: 1\n",
      "Later,: 1\n",
      "Rdd: 2\n",
      "Think: 1\n",
      "First: 2\n",
      "Image: 2\n",
      "phenomena.: 1\n",
      "automatically: 5\n",
      "In-memory: 4\n",
      "managers: 1\n",
      "hungrier: 1\n",
      "fulfilled.: 1\n",
      "down: 2\n",
      "donated: 1\n",
      "exciting,: 1\n",
      "rely: 1\n",
      "support: 2\n",
      "transform: 7\n",
      "systems.: 1\n",
      "way: 1\n",
      "Ubuntu: 3\n",
      "was: 6\n",
      "(java: 1\n",
      "head: 1\n",
      "offering: 2\n",
      "functionality,: 1\n",
      "objects;: 1\n",
      "manager.: 1\n",
      "manager,: 3\n",
      "inside: 2\n",
      "effectively.: 1\n",
      "devices: 1\n",
      "Maven.: 1\n",
      "computing: 4\n",
      "Introduction: 1\n",
      "model1: 3\n",
      "model.: 1\n",
      "model,: 1\n",
      "',: 2\n",
      "check: 8\n",
      "R,: 1\n",
      "when: 7\n",
      "setting: 1\n",
      "np: 1\n",
      "conf/log4j.properties: 2\n",
      "node: 4\n",
      "Windows: 1\n",
      "Later: 1\n",
      "Drive: 1\n",
      "cover,: 1\n",
      "variable: 4\n",
      "transfers: 1\n",
      "rdd.collect(): 1\n",
      "not.: 1\n",
      "faster: 5\n",
      "Reduce: 2\n",
      "variables.: 2\n",
      "variables,: 1\n",
      "time: 12\n",
      "serious: 1\n",
      "skip: 1\n",
      "manager: 1\n",
      "computation: 5\n",
      "row: 2\n",
      "default,: 1\n",
      "environment: 2\n",
      "post.: 1\n",
      "string: 1\n",
      "advantage: 4\n",
      "Next,: 2\n",
      "RAM.: 2\n",
      "exact: 1\n",
      "partitions: 1\n",
      "Just: 1\n",
      "level: 1\n",
      "did: 2\n",
      "turns: 1\n",
      "editor.: 1\n",
      "~/.ipython/profile_default/startup/load_spark_environment_variables.py: 1\n",
      "loads: 1\n",
      "can’t: 2\n",
      "slower: 2\n",
      "prediction: 2\n",
      "(transformed): 1\n",
      "cost: 2\n",
      "domain,: 1\n",
      "Imagine: 1\n",
      "('I',: 1\n",
      "shared: 4\n",
      "tasks.: 1\n",
      "True: 1\n",
      "true,: 1\n",
      "train.describe().show(): 1\n",
      "understanding: 2\n",
      "address: 1\n",
      "say,: 1\n",
      "change: 3\n",
      "tuple..): 1\n",
      "works.: 3\n",
      "here.: 1\n",
      "output:: 2\n",
      "tasks: 4\n",
      "case,: 2\n",
      "extra: 1\n",
      "When: 5\n",
      "output.: 1\n",
      "RandomForestRegressor(): 1\n",
      "take(): 1\n",
      "working: 2\n",
      "value.: 1\n",
      "value,: 1\n",
      "predictions: 4\n",
      "by: 29\n",
      "memory: 2\n",
      "encode: 1\n",
      "mse: 2\n",
      "ways:: 1\n",
      "sharing: 1\n",
      "Benefits: 1\n",
      "These: 1\n",
      "suggestions: 1\n",
      "task.: 2\n",
      "insights: 1\n",
      "select.: 1\n",
      "originally: 1\n",
      "big1: 1\n",
      "scenario,: 1\n",
      "prediction,: 1\n",
      "values: 5\n",
      "can: 54\n",
      "following: 4\n",
      "figure: 1\n",
      "predict: 1\n",
      "predictions1.selectExpr(\"User_ID: 1\n",
      "application,: 1\n",
      "topic: 2\n",
      "performed: 1\n",
      "counting: 1\n",
      "requirements: 1\n",
      "Missing: 1\n",
      "divided: 1\n",
      "means: 4\n",
      "1: 2\n",
      "Source:: 1\n",
      "Gender\",featuresCol=\"features\",labelCol=\"label\"): 1\n",
      "parameter: 2\n",
      "map: 4\n",
      "information: 3\n",
      "applications: 1\n",
      "produce: 1\n",
      "designed: 2\n",
      "category): 1\n",
      "such: 2\n",
      "Model: 1\n",
      "data: 60\n",
      "different.: 1\n",
      "whenever: 1\n",
      "compressing: 1\n",
      "so: 3\n",
      "sc: 3\n",
      "talk: 3\n",
      "“Yes”: 1\n",
      "Step2:: 1\n",
      "below.: 1\n",
      "group: 1\n",
      "file:: 1\n",
      "Pandas.: 1\n",
      "coordinated: 1\n",
      "Once: 4\n",
      "happened: 1\n",
      "Purchase: 1\n",
      "executors: 5\n",
      "introduce: 1\n",
      "not: 28\n",
      "R.: 2\n",
      "now: 2\n",
      "discuss: 2\n",
      "efficiently.: 3\n",
      "range(1,1000): 1\n",
      "execute: 2\n",
      "Scala).: 1\n",
      "name: 2\n",
      "drop: 3\n",
      "entirely: 1\n",
      "square: 2\n",
      "retrieve: 1\n",
      "significantly: 1\n",
      "challenges: 1\n",
      "mechanism.: 1\n",
      "increase: 3\n",
      "Key: 1\n",
      "formula: 6\n",
      "Drive).: 1\n",
      "shows: 1\n",
      "7+,: 1\n",
      "challenge.: 1\n",
      "challenge,: 2\n",
      "dependencies: 1\n",
      "advantages: 2\n",
      "Either: 1\n",
      "com.databricks:spark-csv_2.10:1.3.0: 1\n",
      "language: 2\n",
      "programming: 2\n",
      "turn: 1\n",
      "think: 1\n",
      "lambda: 2\n",
      "Even: 1\n",
      "intuition: 1\n",
      "saving: 1\n",
      "'PATH/test-comb.csv',: 1\n",
      "spoken: 1\n",
      "imputation.: 1\n",
      "variables: 5\n",
      "-version: 1\n",
      "one: 8\n",
      "Black: 2\n",
      "directly: 1\n",
      "open: 5\n",
      "California,: 1\n",
      "millions: 1\n",
      "little: 1\n",
      "categorical: 7\n",
      "returns: 2\n",
      "2: 5\n",
      "typing: 2\n",
      "Data: 3\n",
      "Capturing: 1\n",
      "exploring: 1\n",
      "mostly: 1\n",
      "that: 24\n",
      "on!: 1\n",
      "much.: 1\n",
      "x:: 1\n",
      "Install: 3\n",
      "variable.: 1\n",
      "copy: 2\n",
      "than: 13\n",
      "specify: 4\n",
      "History: 2\n",
      "10: 4\n",
      "Problem:: 1\n",
      "require: 1\n",
      "1:: 1\n",
      "future: 2\n",
      "were: 4\n",
      "rdd.: 1\n",
      "and: 105\n",
      "(we: 1\n",
      "argument: 1\n",
      "recover: 1\n",
      "formula.: 2\n",
      "any: 12\n",
      "conversion: 1\n",
      "efficient: 2\n",
      "ML.: 1\n",
      "note: 2\n",
      "squae: 1\n",
      "Operating: 1\n",
      "take: 4\n",
      "multiple: 5\n",
      "Getting: 1\n",
      "pair: 1\n",
      "evaluator: 2\n",
      "later: 4\n",
      "sources:: 1\n",
      "rf.fit(train1): 1\n",
      "lazy,: 1\n",
      "typically: 2\n",
      "log4j.properties.: 1\n",
      "show: 5\n",
      "ground: 1\n",
      "We: 31\n",
      "only: 7\n",
      "going: 3\n",
      "Organizations: 1\n",
      "get: 5\n",
      "meanwhile,: 1\n",
      "imagine,: 1\n",
      "(csv): 1\n",
      "verbosity: 1\n",
      "sub-tasks: 1\n",
      "summary: 1\n",
      "mapped: 1\n",
      "where: 9\n",
      "printSchema(): 1\n",
      "regressor: 1\n",
      "Systems.: 1\n",
      "(like: 1\n",
      "evaluator.evaluate(predictions,{evaluator.metricName:\"mse\": 1\n",
      "detect: 1\n",
      "ways: 2\n",
      "label: 6\n",
      "(Reduce: 2\n",
      "between: 2\n",
      "import: 11\n",
      "reading: 2\n",
      "across: 4\n",
      "head(): 1\n",
      "Rdd1: 2\n",
      "columns.: 1\n",
      "pyspark.ml.regression: 2\n",
      "article: 4\n",
      "spark: 1\n",
      "come: 4\n",
      "installation: 1\n",
      "fit: 1\n",
      "configuring: 1\n",
      "many: 6\n",
      "But,: 1\n",
      "https://spark.apache.org/docs/1.1.1/img/cluster-overview.png: 1\n",
      "pyspark.ml.evaluation: 1\n",
      "already.: 1\n",
      "comes: 2\n",
      "among: 2\n",
      "period: 1\n",
      "scheduling,: 1\n",
      "64: 1\n",
      "learning: 5\n",
      "constant: 2\n",
      "6:: 1\n",
      "}): 1\n",
      "SparkPi: 1\n",
      "But: 1\n",
      "defined: 3\n",
      "don’t: 1\n",
      "widely,: 1\n",
      "Test1.: 1\n",
      "external: 2\n",
      "Action: 1\n",
      "those: 3\n",
      "case: 3\n",
      "these: 4\n",
      "Open: 1\n",
      "SPARK_HOME=/opt/spark: 1\n",
      "characteristics: 1\n",
      "agnostic: 1\n",
      "drive.: 1\n",
      "cluster: 10\n",
      "drive,: 1\n",
      "Installation: 3\n",
      "Refer: 1\n",
      "different: 7\n",
      "develop: 1\n",
      "train,: 2\n",
      "train.: 3\n",
      "same: 6\n",
      "Alternately,: 1\n",
      "train:: 1\n",
      "finish: 1\n",
      "I: 27\n",
      "driver: 9\n",
      "drives: 1\n",
      "running: 3\n",
      "'PATH/train.csv',: 1\n",
      "log4j.rootCategory=INFO,: 1\n",
      "DVDs: 1\n",
      "It: 13\n",
      "collect(): 1\n",
      "In: 16\n",
      "very: 3\n",
      "model: 8\n",
      "If: 13\n",
      "randomforest: 1\n",
      "actions: 2\n",
      "differentiation: 1\n",
      "(a: 1\n",
      "speed: 5\n",
      "sqlContext: 3\n",
      "directory:: 1\n",
      "(I: 1\n",
      "4: 1\n",
      "guide.: 1\n",
      "around: 1\n",
      "read: 4\n",
      "PYTHONPATH: 1\n",
      "executors.: 1\n",
      "test: 12\n",
      "action.: 3\n",
      "JVM: 1\n",
      "world: 1\n",
      "cd: 3\n",
      "\"Product_ID: 1\n",
      "PYSPARK_DRIVER_PYTHON=ipython: 1\n",
      "reduces: 1\n",
      "either: 1\n",
      "output: 2\n",
      "Since: 1\n",
      "computations,: 1\n",
      "competition: 1\n",
      "Py4J: 1\n",
      "specified: 1\n",
      "regressor,: 1\n",
      "provides: 1\n",
      "t1.transform(Train1): 1\n",
      "2010.: 1\n",
      "assembly: 1\n",
      "communicate: 1\n",
      "update: 1\n",
      "'I': 1\n",
      "Impute: 1\n",
      "refers: 1\n",
      "on: 39\n",
      "package: 3\n",
      "of: 122\n",
      "discussed: 3\n",
      "Analyzing: 2\n",
      "HBase,: 1\n",
      "os: 1\n",
      "or: 9\n",
      "communication: 1\n",
      "Format.: 1\n",
      "Add: 1\n",
      "https://en.wikipedia.org/wiki/Memory_hierarchy: 1\n",
      "your: 5\n",
      "there: 6\n",
      "start: 8\n",
      "lot: 3\n",
      "'/opt/spark/python': 1\n",
      "0.3]): 1\n",
      "Remember: 1\n",
      "beginners: 1\n",
      "RDD: 26\n",
      "another.: 1\n",
      "technologies: 1\n",
      "with: 32\n",
      "happening: 2\n",
      "distict: 1\n",
      "applying: 12\n",
      "train1,Test1: 1\n",
      "default: 1\n",
      "impute: 1\n",
      "shell,: 2\n",
      "shell.: 1\n",
      "connect: 1\n",
      "strongly: 2\n",
      "pyspark.ml.feature: 2\n",
      "certain: 3\n",
      "am: 11\n",
      "an: 16\n",
      "How: 2\n",
      "Lets: 9\n",
      "at: 14\n",
      "file: 9\n",
      "fill: 1\n",
      "string.: 1\n",
      "again: 1\n",
      "Read: 1\n",
      "storage: 2\n",
      "back,: 1\n",
      "you: 29\n",
      "rf.fit(train_cv): 2\n",
      "briefly: 1\n",
      "Cyclic: 1\n",
      "important: 1\n",
      "itself.: 1\n",
      "included: 1\n",
      "building: 1\n",
      "storage.: 2\n",
      "typing:: 1\n",
      "14.04,: 1\n",
      "storage:: 2\n",
      "driver.: 1\n",
      "directory: 1\n",
      "typing.: 1\n",
      "library.: 1\n",
      "disadvantages.: 1\n",
      "'product_ID'): 1\n",
      "plan_indexer: 1\n",
      "all: 8\n",
      "elements,: 2\n",
      "executing: 1\n",
      "follow: 2\n",
      "disk: 9\n",
      "Otherwise,: 1\n",
      "-i: 1\n",
      "to: 152\n",
      "program: 5\n",
      "-s: 1\n",
      "nodes: 4\n",
      "immutable: 1\n",
      "http://www.scala-lang.org/files/archive/scala-2.11.7.deb: 1\n",
      "sparkContext: 1\n",
      "earlier.: 1\n",
      "Worker:: 1\n",
      "far: 1\n",
      "results: 5\n",
      "'AM',: 1\n",
      "(word,1): 1\n",
      "difference: 2\n",
      "2.6+,: 1\n",
      "/opt/spark-1.6.0: 1\n",
      "node,: 1\n",
      "node.: 2\n",
      "list: 2\n",
      "large: 6\n",
      "library),: 1\n",
      "-1: 2\n",
      "along,: 1\n",
      "(Resilient: 1\n",
      "‘labeller’: 2\n",
      "hour.: 1\n",
      "past: 1\n",
      "3.1+: 1\n",
      "Purchase'): 1\n",
      "further: 2\n",
      "Framework?: 1\n",
      "“spark-csv_2.10:1.3.0”).: 1\n",
      "RDDs:: 1\n",
      "what: 4\n",
      "nano: 3\n",
      "version: 4\n",
      "method: 13\n",
      "full: 1\n",
      "Step1:: 1\n",
      "Product_ID\",: 1\n",
      "predictions1: 1\n",
      "predictions.: 1\n",
      "allows: 1\n",
      "prior: 1\n",
      "train.select('Product_ID').distinct().count(),: 1\n",
      "real: 3\n",
      "options: 1\n",
      "train1.show(): 1\n",
      "analyzing: 1\n",
      "viz: 1\n",
      "sensors: 2\n",
      "select: 4\n",
      "Broadcast: 2\n",
      "distinct: 2\n",
      "contains: 2\n",
      "two: 3\n",
      "more: 13\n",
      "knows: 1\n",
      "presumes: 1\n",
      "Above,: 1\n",
      "So: 1\n",
      "stick: 1\n",
      "particular: 1\n",
      "known: 4\n",
      "approximated.: 1\n",
      "keeping: 1\n",
      "Now: 5\n",
      "science: 3\n",
      "take:: 1\n",
      "installing: 3\n",
      "resources: 1\n",
      "learn: 4\n",
      "(with: 1\n",
      "Blue-ray: 1\n",
      "share: 3\n",
      "accept: 1\n",
      "numbers: 1\n",
      "sense: 1\n",
      "plan.: 1\n",
      "huge: 3\n",
      "Variable:: 1\n",
      "rather: 1\n",
      "('AM',: 1\n",
      "steps:: 1\n",
      "Spark: 68\n",
      "csv: 8\n",
      "simplicity: 1\n",
      "(Latest): 1\n",
      "coming: 2\n",
      "a: 112\n",
      "SparkR,: 1\n",
      "element: 4\n",
      "immutable,: 1\n",
      "Note:: 1\n",
      "What: 4\n",
      "help: 5\n",
      "developed: 1\n",
      "installed: 4\n",
      "count().: 2\n",
      "through: 3\n",
      "labelCol=”label”).: 1\n",
      "its: 4\n",
      "PySpark,: 3\n",
      "PySpark.: 1\n",
      "won’t: 1\n",
      "(computation): 1\n",
      "actually: 2\n",
      "systems: 2\n",
      "initializing: 1\n",
      "might: 1\n",
      "train1: 7\n",
      "it.: 2\n",
      "it,: 1\n",
      "return: 1\n",
      "it!: 1\n",
      "framework: 1\n",
      "Berkeley’s: 1\n",
      "Spark-csv: 1\n",
      "fine.: 1\n",
      "key).: 1\n",
      "system,: 2\n",
      "Disk: 1\n",
      "End: 1\n",
      "generation: 2\n",
      "hard: 4\n",
      "terminal.: 1\n",
      "Build: 1\n",
      "operation: 5\n",
      "Reading: 1\n",
      "really: 2\n",
      "since: 1\n",
      "print: 1\n",
      "Required:: 1\n",
      "evaluation: 3\n",
      "sbt: 1\n",
      "base: 3\n",
      "imagine: 2\n",
      "ask: 2\n",
      "thread: 1\n",
      "launch: 1\n",
      "categories(invalid: 1\n",
      "Huge: 1\n",
      "utilizes: 1\n",
      "feel: 1\n",
      "number: 14\n",
      "well-known: 1\n",
      "minutes.: 1\n",
      "Sub-setting: 1\n",
      "temperature: 1\n",
      "script: 1\n",
      "introduction: 1\n",
      "programmer).: 1\n",
      "interact: 1\n",
      "header.: 1\n",
      "(illustrated: 1\n",
      "SQLContext.: 1\n",
      "store: 3\n",
      "schema: 1\n",
      "Language: 1\n",
      "[('Hello',: 1\n",
      "part: 1\n",
      "information.: 1\n",
      "dpkg: 1\n",
      "Model:: 1\n",
      "kind: 1\n",
      "machines,: 1\n",
      "remembers: 1\n",
      "model.transform(test1): 1\n",
      "experience.: 1\n",
      "roughly: 1\n",
      "null: 6\n",
      "folder,: 1\n",
      "Challenges: 6\n",
      "depending: 1\n",
      "Suppose,: 1\n",
      "also: 16\n",
      "build: 4\n",
      "analogy: 1\n",
      "Analysing: 1\n",
      "folders: 1\n",
      "added: 2\n",
      "programmers: 1\n",
      "that.: 3\n",
      "most: 2\n",
      "Common: 1\n",
      "nothing: 2\n",
      "statistics: 1\n",
      "The: 13\n",
      "7:: 1\n",
      "traditional: 4\n",
      "AMPLab: 1\n",
      "distinct(): 1\n",
      "path.: 1\n",
      "py4j: 2\n",
      "find: 2\n",
      "with.: 1\n",
      "Then,: 1\n",
      "dividing: 1\n",
      "distributed: 7\n",
      "shuffling.: 1\n",
      "rf.: 1\n",
      "columns: 8\n",
      "conf/log4j.properties.template: 1\n",
      "dependent: 1\n",
      "compiled: 1\n",
      "flowing: 1\n",
      "Transformation:: 1\n",
      "sbt/sbt: 1\n",
      "famous: 1\n",
      "feels: 1\n",
      "valuable: 1\n",
      "weeks,: 1\n",
      "comma.: 1\n",
      "restart: 1\n",
      "column.: 3\n",
      "querying: 2\n",
      "‘Rdd’): 1\n",
      "Random: 1\n",
      "stored: 1\n",
      "common: 2\n",
      "parameter,: 1\n",
      "set: 8\n",
      "For: 16\n",
      "Create: 4\n",
      "see: 27\n",
      "are: 50\n",
      "file(submission.csv).: 1\n",
      "#: 1\n",
      "Hence,: 1\n",
      "test.select('Product_ID').distinct().count(): 1\n",
      "currently: 1\n",
      "1)]: 1\n",
      "years,: 1\n",
      "probably: 1\n",
      "available: 1\n",
      "1),: 4\n",
      "creating: 6\n",
      "pyspark.ml.feature.: 1\n",
      "importing,: 1\n",
      "ipython: 2\n",
      "console: 2\n",
      "both: 1\n",
      "last: 1\n",
      "reverse: 1\n",
      "license: 1\n",
      "Hard: 2\n",
      "non-overlapping: 1\n",
      "connection: 1\n",
      "above,: 1\n",
      "load: 3\n",
      "computers.: 1\n",
      "simple: 2\n",
      "(featuresCol=”features”: 1\n",
      "header: 2\n",
      "scala-2.11.7.deb: 1\n",
      "simply: 1\n",
      "java: 1\n",
      "transformations: 3\n",
      "format.: 1\n",
      "RFormula(formula=\"Purchase: 1\n",
      "Most: 1\n",
      "collected: 1\n",
      "in-place: 1\n",
      "understand: 3\n",
      "prices: 1\n",
      "Software: 1\n",
      "look: 5\n",
      "pace: 2\n",
      "while: 5\n",
      "error: 1\n",
      "transformation.: 1\n",
      "Context:: 1\n",
      "costly: 1\n",
      "labeller.transform(test): 1\n",
      "use.: 1\n",
      "reads: 2\n",
      "itself: 2\n",
      "transom: 1\n",
      "outputCol: 1\n",
      "(Action),: 1\n",
      "So,: 1\n",
      "'SPARK_HOME': 1\n",
      "widely: 1\n",
      "Example:: 1\n",
      "higher: 1\n",
      "used: 9\n",
      "forced: 1\n",
      "comprehensive: 1\n",
      "sys: 1\n",
      "moment: 1\n",
      "uses: 2\n",
      "aggregate: 1\n",
      "lower: 1\n",
      "task: 9\n",
      "Mesos: 1\n",
      "wget:: 1\n",
      "(some: 1\n",
      "convenient: 1\n",
      "spend: 2\n",
      "nodes.: 1\n",
      "Java.: 1\n",
      "computing,: 2\n",
      "computing.: 1\n",
      "(because: 1\n",
      "questions: 1\n",
      "using: 11\n",
      "Although,: 1\n",
      "$: 25\n",
      "developed,: 1\n",
      "extracted: 1\n",
      "recommended: 1\n",
      "source: 5\n",
      "location: 1\n",
      "disk.: 2\n",
      "transformation: 10\n",
      "“Product_ID”: 1\n",
      "On: 1\n",
      "faster,: 1\n",
      "format: 2\n",
      "evaluate: 4\n",
      "showing: 1\n",
      "emanating: 1\n",
      "plan_indexer.fit(train): 1\n",
      "bit: 2\n",
      "projects: 1\n",
      "submission.: 2\n",
      "Computing: 2\n",
      "collect: 2\n",
      "continue: 1\n",
      "worker,: 1\n",
      "Learning: 2\n",
      "popular: 1\n",
      "table.: 1\n",
      "table,: 1\n",
      "Train1: 5\n",
      "parallelized).: 1\n",
      "Java: 8\n",
      "methods: 2\n",
      "some: 13\n",
      "back: 3\n",
      "examples: 2\n",
      "apply: 8\n",
      "scale: 3\n",
      "processing,: 3\n",
      "processing.: 2\n",
      "scala: 1\n",
      "transactions: 1\n",
      "processing:: 1\n",
      "minimal: 1\n",
      "be: 13\n",
      "run: 6\n",
      "example,: 8\n",
      "processing: 8\n",
      "step: 4\n",
      "I’ll: 3\n",
      "method.: 6\n",
      "wget: 2\n",
      "method,: 2\n",
      "example:: 1\n",
      "30%: 1\n",
      "analysing: 1\n",
      "User_ID\",: 1\n",
      "PYTHONPATH=$SPARK_HOME/python: 1\n",
      "into: 4\n",
      "Shared: 1\n",
      "ways.: 1\n",
      "file.: 1\n",
      "file,: 1\n",
      "rows: 5\n",
      "retrieving: 1\n",
      "question: 1\n",
      "'Ankit: 1\n",
      "fast: 5\n",
      "os.environ:: 1\n",
      ":: 1\n",
      "sections: 1\n",
      "files: 4\n",
      "frame.: 1\n",
      "link: 2\n",
      "noticed,: 1\n",
      "('Gupta',: 1\n",
      "up: 8\n",
      "us: 10\n",
      "'/opt/spark/python'): 1\n",
      "continuously: 1\n",
      "similar: 1\n",
      "called: 7\n",
      "partition: 3\n",
      "associated: 2\n",
      "problem:: 1\n",
      "MapReduce.: 1\n",
      "sizes,: 1\n",
      "To: 12\n",
      "single: 3\n",
      "Above: 1\n",
      "series.: 1\n",
      "Dataset.: 1\n",
      "Map: 4\n",
      "amounts: 1\n",
      "application: 3\n",
      "Mac: 1\n",
      "functionality: 1\n",
      "Accumulator:: 1\n",
      "pandas: 2\n",
      "elements: 1\n",
      "Querying: 1\n",
      "“Practice: 1\n",
      "aggregates: 1\n",
      "Mapped: 1\n",
      "independently: 1\n",
      "tar.: 1\n",
      "3.14042: 1\n",
      "required: 4\n",
      "depth: 1\n",
      "Dataframe,: 5\n",
      "At: 2\n",
      "requires: 2\n",
      "once: 1\n",
      "True,: 1\n",
      "code: 3\n",
      "True): 2\n",
      "aggregating: 1\n",
      "RegressionEvaluator: 2\n",
      "existing: 5\n",
      "rdd.take(2): 1\n",
      "query: 1\n",
      "partitions.: 1\n",
      "spark-1.6.0.tgz: 1\n",
      "send: 2\n",
      "languages: 1\n",
      "Step: 10\n",
      "helps: 1\n",
      "position,: 1\n",
      "include: 2\n",
      "sent: 2\n",
      "random: 2\n",
      "1.6.0.: 1\n",
      "telling: 1\n",
      "apt-get: 2\n",
      "categories: 4\n",
      "entire: 1\n",
      "train.na.drop().count(),test.na.drop('any').count(): 1\n",
      "hierarchy,: 1\n",
      "notes: 1\n",
      "language.: 3\n",
      "try: 1\n",
      "“Batch: 1\n",
      "dealt: 1\n",
      "refer: 2\n",
      "other.: 1\n",
      "/opt/spark: 1\n",
      "download: 3\n",
      "Transformation: 4\n",
      "Python,: 1\n",
      "optimize: 3\n",
      "Python.: 2\n",
      "Python): 1\n",
      "df.toPandas().to_csv('submission.csv'): 1\n",
      "access: 2\n",
      "(If: 1\n",
      "Dataframes.: 1\n",
      "Serialization: 1\n",
      "let: 2\n",
      "os.environ['SPARK_HOME']: 1\n",
      "Broadcast:: 1\n",
      "3:: 2\n",
      "convert: 1\n",
      "parameters: 2\n",
      "rdd: 2\n",
      "resulting: 1\n",
      "implement: 1\n",
      "SparkContext(): 1\n",
      "Dataframes: 2\n",
      "below).: 1\n",
      "names: 1\n",
      "model1.transform(test_cv): 2\n",
      "Input: 1\n",
      "cloud: 1\n",
      "use: 14\n",
      "from: 28\n",
      "representation,: 1\n",
      "Driver: 1\n",
      "Manager:: 1\n",
      "next: 2\n",
      "few: 3\n",
      "(transformations: 1\n",
      "formula.fit(Train1): 1\n",
      "~/.bashrc: 2\n",
      "labeller: 1\n",
      "comparison: 1\n",
      "started: 2\n",
      "name,: 1\n",
      "'Gupta']: 1\n",
      "Dataframe.: 3\n",
      "train: 16\n",
      "test_cv.: 3\n",
      "sqlContext.: 1\n",
      "customer: 1\n",
      "this: 50\n",
      "challenge: 1\n",
      "Qunitillion: 1\n",
      "tar: 1\n",
      "process: 8\n",
      "sudo: 7\n",
      "high: 2\n",
      "effectively: 1\n",
      "native: 1\n",
      "computation:: 1\n",
      "Subsequently,: 1\n",
      "line:: 1\n",
      "holds: 1\n",
      "df: 2\n",
      "forest: 2\n",
      "“Distributed: 1\n",
      "instead: 1\n",
      "train.head(10): 1\n",
      "collection: 3\n",
      "Batch: 1\n",
      "notebook: 1\n",
      "(Hadoop: 1\n",
      "link.: 2\n",
      "computations: 4\n",
      "lines: 2\n",
      "While: 2\n",
      "Dataset: 4\n",
      "printSchema().: 1\n",
      "“Cluster”.: 1\n",
      "hand,: 2\n",
      "hand.: 1\n",
      "interpreted: 1\n",
      "including: 2\n",
      "converting: 1\n",
      "write: 1\n",
      "ln: 1\n",
      "–version: 1\n",
      "RAM: 6\n",
      "Output:(3631,: 1\n",
      "velocity.: 1\n",
      "clusters: 1\n",
      "article.: 2\n",
      "article,: 4\n",
      "describe(): 1\n",
      "python: 2\n",
      "computation,: 1\n",
      "test_cv): 1\n",
      "Can: 1\n",
      "However,: 6\n",
      "let’s: 1\n",
      "cluster.: 4\n",
      "cluster,: 3\n",
      "Statistics: 1\n",
      "doing: 5\n",
      "8:: 1\n",
      "books: 1\n",
      "Which: 1\n",
      "Print: 1\n",
      "our: 13\n",
      "out: 4\n",
      "product_ID: 1\n",
      "Selecting: 1\n",
      "incharge: 1\n",
      "time,: 2\n",
      "time.: 2\n",
      "performs: 2\n",
      "supports: 5\n",
      "This: 14\n",
      "show(): 1\n",
      "-1.: 1\n",
      "completely: 1\n",
      "operation),: 1\n",
      "completion.: 1\n",
      "Computing”: 1\n",
      "could: 1\n",
      "times: 3\n",
      "representations:: 1\n",
      "imputing: 3\n",
      "Standalone,: 1\n",
      "documentation,: 1\n",
      "‘User_ID’: 1\n",
      "representations: 1\n",
      "has: 22\n",
      "labeller.: 1\n",
      "final: 1\n",
      "interests: 1\n",
      "shell: 3\n",
      "oracle-java7-installer: 1\n",
      "exactly: 1\n",
      "R: 3\n",
      "Existing: 1\n",
      "log4j.rootCategory=ERROR,: 1\n",
      "dataset.: 1\n",
      "providing: 2\n",
      "subtract: 1\n",
      "/opt/: 1\n",
      "Machine: 3\n",
      "have: 25\n",
      "need: 27\n",
      "PATH: 1\n",
      "One: 2\n",
      "complete: 1\n",
      "1.3.: 1\n",
      "Action:: 1\n",
      "After: 11\n",
      "Action.: 1\n",
      "which: 38\n",
      "=: 41\n",
      "mse.: 1\n",
      "Cluster.: 1\n",
      "Product_ID: 3\n",
      "who: 1\n",
      "in-memory: 4\n",
      "installing,: 1\n",
      "connected: 1\n",
      "inferSchema: 2\n",
      "RFormula: 3\n",
      "Actions: 1\n",
      "actions).: 1\n",
      "train1,: 1\n",
      "mapping.: 1\n",
      "normally: 1\n",
      "fact: 1\n",
      "SQLContext(sc): 1\n",
      "supported: 2\n",
      "generation,: 1\n",
      "generation.: 1\n",
      "based: 5\n",
      "knowledge: 1\n",
      "SparkContext.: 2\n",
      "should: 1\n",
      "various: 3\n",
      "time),: 1\n",
      "local: 1\n",
      "interested: 1\n",
      "meant: 1\n",
      "handle: 2\n",
      "pyspark: 3\n",
      "Then: 1\n",
      "YARN.: 1\n",
      "areas: 1\n",
      "Move: 1\n",
      "calling: 1\n",
      "processed: 1\n",
      "fit(): 1\n",
      "requirement: 1\n",
      "frame: 2\n",
      "we: 72\n",
      "–: 4\n",
      "(number: 1\n",
      "Pi: 1\n",
      "end,: 1\n",
      "acquires: 2\n",
      "Again,: 1\n",
      "written: 3\n",
      "correctly: 1\n",
      "operations,: 2\n",
      "operations.: 1\n",
      "ends: 1\n",
      "opening: 1\n",
      "3491): 1\n",
      "configuration: 1\n",
      "problem: 2\n",
      "commands: 2\n",
      "MapReduce: 7\n",
      "clusters.: 1\n",
      "clusters,: 1\n",
      "passing: 2\n",
      "values,: 1\n",
      "Contents: 1\n",
      "Solving: 2\n",
      "allocates: 1\n",
      "Steps:: 1\n",
      "rows): 1\n",
      "cp: 1\n",
      "framework.: 3\n",
      "framework,: 1\n",
      "framework?: 1\n",
      "Compile: 1\n",
      "CDs: 1\n",
      "present: 1\n",
      "And: 1\n",
      "train1.select('label').show(): 1\n",
      "as: 28\n",
      "value: 1\n",
      "choices: 1\n",
      "will: 32\n",
      "frameworks: 4\n",
      "~: 1\n",
      "sys.path.insert(0,: 1\n",
      "manually: 1\n",
      "vs: 2\n",
      "You: 2\n",
      "capture: 1\n",
      "cross: 1\n",
      "Paste: 1\n",
      "difficult: 2\n",
      "transformations.: 1\n",
      "Real: 1\n",
      "upon: 1\n",
      "Facebook,: 1\n",
      "asked: 1\n",
      "StringIndexer(inputCol: 1\n",
      "command: 1\n",
      "sets: 1\n",
      "latest: 1\n",
      "test1: 3\n",
      "sourced: 1\n",
      "less: 1\n",
      "click: 1\n",
      "sources: 2\n",
      "underlying: 1\n",
      "rooms: 1\n",
      "rest: 1\n",
      "Analyze: 1\n",
      "optimization.: 1\n",
      "test.: 4\n",
      "work,: 2\n",
      "work.: 1\n",
      "Transforming: 1\n",
      "Friday: 1\n",
      "(name: 1\n",
      "Output:: 1\n",
      "add: 3\n",
      "validation.: 1\n",
      "logger: 1\n",
      "By: 2\n",
      "python.: 1\n",
      "room,: 1\n",
      "source.: 1\n",
      "source,: 2\n",
      "know: 2\n",
      "source:: 1\n",
      "necessary: 1\n",
      "like: 8\n",
      "actions:: 1\n",
      "StringIndexer: 2\n",
      "(or: 1\n",
      "--packages: 2\n",
      "become: 1\n",
      "replacement: 1\n",
      "drop(): 2\n",
      "because: 4\n",
      "Scala: 11\n",
      "library: 1\n",
      "export: 3\n",
      "past,: 1\n",
      "home: 1\n",
      "does: 1\n",
      "Age+: 1\n",
      "Columns: 1\n",
      "results.: 2\n",
      "OS:: 1\n",
      "(x,1)): 1\n",
      "train.printSchema(): 1\n",
      "gained: 1\n",
      "about: 12\n",
      "column: 13\n",
      "introduced: 1\n",
      "create: 16\n",
      "own: 1\n",
      "Context: 2\n",
      "parallelize: 3\n",
      "goal.: 1\n",
      "Writing: 1\n",
      "inmemory: 1\n",
      "weather: 2\n",
      "['Hello': 1\n",
      "t1.transform(Test1): 1\n",
      "indices.: 1\n",
      "First,: 1\n",
      "settings,: 1\n",
      "settings.: 1\n",
      "Friday”: 1\n",
      "score,: 1\n",
      "biggest: 1\n",
      "Easiest: 1\n",
      "representations,: 1\n",
      "function: 4\n",
      "but: 9\n",
      "volume: 1\n",
      "RandomForestRegressor: 1\n",
      "editing: 1\n",
      "count: 3\n",
      "Distributing: 1\n",
      "(-1).: 1\n",
      "made: 1\n",
      "whether: 3\n",
      "numpy: 1\n",
      "train1.randomSplit([0.7,: 1\n",
      "this.: 1\n",
      "Frameworks.: 1\n",
      "demonstrate: 2\n",
      "distribution: 2\n",
      "70%: 1\n",
      "pip: 1\n",
      "train.count(): 1\n",
      "functions: 1\n",
      "DAG: 2\n",
      "4:: 1\n",
      "46: 2\n",
      "Million: 1\n",
      "useful,: 1\n",
      "virtual: 1\n",
      "options.: 1\n",
      "other: 5\n",
      "details: 1\n",
      "train1,test1.: 1\n",
      "proceed.: 1\n",
      "repeat: 1\n",
      "in:: 1\n",
      "Graph).: 1\n",
      "possible).: 1\n",
      "friends: 1\n",
      "needed: 1\n",
      "'Product_ID',: 1\n",
      "capturing:: 1\n"
     ]
    }
   ],
   "source": [
    "#Wordcount\n",
    "import sys\n",
    "from operator import add\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    #Reading the file - SQL DF\n",
    "    #lines = spark.read.text('file:///home/vagrant/SparkPractices/Share/blogtexts')\n",
    "    lines = spark.read.text('D:\\\\Windows\\\\Documents\\\\GitHub\\\\Hadoop\\\\Share\\\\blogtexts')\n",
    "    print(type(lines))\n",
    "    \n",
    "    #Turn it into a RDD\n",
    "    line=lines.rdd.map(lambda r: r[0])\n",
    "    \n",
    "    print(type(line))\n",
    "    print(line.take(5))\n",
    "    \n",
    "    #Steps to count words\n",
    "    #STEP 1 - FLATMAP - Get 1 line per row\n",
    "    flatmap1= line.flatMap(lambda x: x.split(' ')) \n",
    "    \n",
    "    print(flatmap1.take(3))\n",
    "    \n",
    "    #STEP 2 - MAP\n",
    "    map1=flatmap1.map(lambda x: (x, 1))\n",
    "    \n",
    "    print(map1.take(5))\n",
    "    \n",
    "    #STEP 3 - REDUCEBYKEY to add number...KEY is the word\n",
    "    counts=map1.reduceByKey(add)\n",
    "    \n",
    "    print(counts.take(3))\n",
    "    \n",
    "    #Consolidating the output\n",
    "    output = counts.collect()\n",
    "    \n",
    "    for (word, count) in output:\n",
    "        print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is an example implementation of ALS for learning how to use Spark. Please refer to\n",
    "pyspark.ml.recommendation.ALS for more conventional use.\n",
    "This example requires numpy (http://www.numpy.org/)\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "from numpy import matrix\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "LAMBDA = 0.01   # regularization\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def rmse(R, ms, us):\n",
    "    diff = R - ms * us.T\n",
    "    return np.sqrt(np.sum(np.power(diff, 2)) / (M * U))\n",
    "\n",
    "\n",
    "def update(i, mat, ratings):\n",
    "    uu = mat.shape[0]\n",
    "    ff = mat.shape[1]\n",
    "\n",
    "    XtX = mat.T * mat\n",
    "    Xty = mat.T * ratings[i, :].T\n",
    "\n",
    "    for j in range(ff):\n",
    "        XtX[j, j] += LAMBDA * uu\n",
    "\n",
    "    return np.linalg.solve(XtX, Xty)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    \"\"\"\n",
    "    Usage: als [M] [U] [F] [iterations] [partitions]\"\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\"\"WARN: This is a naive implementation of ALS and is given as an\n",
    "      example. Please use pyspark.ml.recommendation.ALS for more\n",
    "      conventional use.\"\"\", file=sys.stderr)\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonALS\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    M = int(sys.argv[1]) if len(sys.argv) > 1 else 100\n",
    "    U = int(sys.argv[2]) if len(sys.argv) > 2 else 500\n",
    "    F = int(sys.argv[3]) if len(sys.argv) > 3 else 10\n",
    "    ITERATIONS = int(sys.argv[4]) if len(sys.argv) > 4 else 5\n",
    "    partitions = int(sys.argv[5]) if len(sys.argv) > 5 else 2\n",
    "\n",
    "    print(\"Running ALS with M=%d, U=%d, F=%d, iters=%d, partitions=%d\\n\" %\n",
    "          (M, U, F, ITERATIONS, partitions))\n",
    "\n",
    "    R = matrix(rand(M, F)) * matrix(rand(U, F).T)\n",
    "    ms = matrix(rand(M, F))\n",
    "    us = matrix(rand(U, F))\n",
    "\n",
    "    Rb = sc.broadcast(R)\n",
    "    msb = sc.broadcast(ms)\n",
    "    usb = sc.broadcast(us)\n",
    "\n",
    "    for i in range(ITERATIONS):\n",
    "        ms = sc.parallelize(range(M), partitions) \\\n",
    "               .map(lambda x: update(x, usb.value, Rb.value)) \\\n",
    "               .collect()\n",
    "        # collect() returns a list, so array ends up being\n",
    "        # a 3-d array, we take the first 2 dims for the matrix\n",
    "        ms = matrix(np.array(ms)[:, :, 0])\n",
    "        msb = sc.broadcast(ms)\n",
    "\n",
    "        us = sc.parallelize(range(U), partitions) \\\n",
    "               .map(lambda x: update(x, msb.value, Rb.value.T)) \\\n",
    "               .collect()\n",
    "        us = matrix(np.array(us)[:, :, 0])\n",
    "        usb = sc.broadcast(us)\n",
    "\n",
    "        error = rmse(R, ms, us)\n",
    "        print(\"Iteration %d:\" % i)\n",
    "        print(\"\\nRMSE: %5.4f\\n\" % error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read data file users.avro in local Spark distro:\n",
    "$ cd $SPARK_HOME\n",
    "$ ./bin/spark-submit --driver-class-path /path/to/example/jar \\\n",
    "> ./examples/src/main/python/avro_inputformat.py \\\n",
    "> examples/src/main/resources/users.avro\n",
    "{u'favorite_color': None, u'name': u'Alyssa', u'favorite_numbers': [3, 9, 15, 20]}\n",
    "{u'favorite_color': u'red', u'name': u'Ben', u'favorite_numbers': []}\n",
    "To read name and favorite_color fields only, specify the following reader schema:\n",
    "$ cat examples/src/main/resources/user.avsc\n",
    "{\"namespace\": \"example.avro\",\n",
    " \"type\": \"record\",\n",
    " \"name\": \"User\",\n",
    " \"fields\": [\n",
    "     {\"name\": \"name\", \"type\": \"string\"},\n",
    "     {\"name\": \"favorite_color\", \"type\": [\"string\", \"null\"]}\n",
    " ]\n",
    "}\n",
    "$ ./bin/spark-submit --driver-class-path /path/to/example/jar \\\n",
    "> ./examples/src/main/python/avro_inputformat.py \\\n",
    "> examples/src/main/resources/users.avro examples/src/main/resources/user.avsc\n",
    "{u'favorite_color': None, u'name': u'Alyssa'}\n",
    "{u'favorite_color': u'red', u'name': u'Ben'}\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "from functools import reduce\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2 and len(sys.argv) != 3:\n",
    "        print(\"\"\"\n",
    "        Usage: avro_inputformat <data_file> [reader_schema_file]\n",
    "        Run with example jar:\n",
    "        ./bin/spark-submit --driver-class-path /path/to/example/jar \\\n",
    "        /path/to/examples/avro_inputformat.py <data_file> [reader_schema_file]\n",
    "        Assumes you have Avro data stored in <data_file>. Reader schema can be optionally specified\n",
    "        in [reader_schema_file].\n",
    "        \"\"\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    path = sys.argv[1]\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"AvroKeyInputFormat\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    conf = None\n",
    "    if len(sys.argv) == 3:\n",
    "        schema_rdd = sc.textFile(sys.argv[2], 1).collect()\n",
    "        conf = {\"avro.schema.input.key\": reduce(lambda x, y: x + y, schema_rdd)}\n",
    "\n",
    "    avro_rdd = sc.newAPIHadoopFile(\n",
    "        path,\n",
    "        \"org.apache.avro.mapreduce.AvroKeyInputFormat\",\n",
    "        \"org.apache.avro.mapred.AvroKey\",\n",
    "        \"org.apache.hadoop.io.NullWritable\",\n",
    "        keyConverter=\"org.apache.spark.examples.pythonconverters.AvroWrapperToJavaConverter\",\n",
    "        conf=conf)\n",
    "    output = avro_rdd.map(lambda x: x[0]).collect()\n",
    "    for k in output:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The K-means algorithm written from scratch against PySpark. In practice,\n",
    "one may prefer to use the KMeans algorithm in ML, as shown in\n",
    "examples/src/main/python/ml/kmeans_example.py.\n",
    "This example requires NumPy (http://www.numpy.org/).\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def parseVector(line):\n",
    "    return np.array([float(x) for x in line.split(' ')])\n",
    "\n",
    "\n",
    "def closestPoint(p, centers):\n",
    "    bestIndex = 0\n",
    "    closest = float(\"+inf\")\n",
    "    for i in range(len(centers)):\n",
    "        tempDist = np.sum((p - centers[i]) ** 2)\n",
    "        if tempDist < closest:\n",
    "            closest = tempDist\n",
    "            bestIndex = i\n",
    "    return bestIndex\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if len(sys.argv) != 4:\n",
    "        print(\"Usage: kmeans <file> <k> <convergeDist>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    print(\"\"\"WARN: This is a naive implementation of KMeans Clustering and is given\n",
    "       as an example! Please refer to examples/src/main/python/ml/kmeans_example.py for an\n",
    "       example on how to use ML's KMeans implementation.\"\"\", file=sys.stderr)\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonKMeans\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
    "    data = lines.map(parseVector).cache()\n",
    "    K = int(sys.argv[2])\n",
    "    convergeDist = float(sys.argv[3])\n",
    "\n",
    "    kPoints = data.takeSample(False, K, 1)\n",
    "    tempDist = 1.0\n",
    "\n",
    "    while tempDist > convergeDist:\n",
    "        closest = data.map(\n",
    "            lambda p: (closestPoint(p, kPoints), (p, 1)))\n",
    "        pointStats = closest.reduceByKey(\n",
    "            lambda p1_c1, p2_c2: (p1_c1[0] + p2_c2[0], p1_c1[1] + p2_c2[1]))\n",
    "        newPoints = pointStats.map(\n",
    "            lambda st: (st[0], st[1][0] / st[1][1])).collect()\n",
    "\n",
    "        tempDist = sum(np.sum((kPoints[iK] - p) ** 2) for (iK, p) in newPoints)\n",
    "\n",
    "        for (iK, p) in newPoints:\n",
    "            kPoints[iK] = p\n",
    "\n",
    "    print(\"Final centers: \" + str(kPoints))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A logistic regression implementation that uses NumPy (http://www.numpy.org)\n",
    "to act on batches of input data using efficient matrix operations.\n",
    "\n",
    "In practice, one may prefer to use the LogisticRegression algorithm in\n",
    "ML, as shown in examples/src/main/python/ml/logistic_regression_with_elastic_net.py.\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "D = 10  # Number of dimensions\n",
    "\n",
    "\n",
    "# Read a batch of points from the input file into a NumPy matrix object. We operate on batches to\n",
    "# make further computations faster.\n",
    "# The data file contains lines of the form <label> <x1> <x2> ... <xD>. We load each block of these\n",
    "# into a NumPy array of size numLines * (D + 1) and pull out column 0 vs the others in gradient().\n",
    "def readPointBatch(iterator):\n",
    "    strs = list(iterator)\n",
    "    matrix = np.zeros((len(strs), D + 1))\n",
    "    for i, s in enumerate(strs):\n",
    "        matrix[i] = np.fromstring(s.replace(',', ' '), dtype=np.float32, sep=' ')\n",
    "    return [matrix]\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: logistic_regression <file> <iterations>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    print(\"\"\"WARN: This is a naive implementation of Logistic Regression and is\n",
    "      given as an example!\n",
    "      Please refer to examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
    "      to see how ML's implementation is used.\"\"\", file=sys.stderr)\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonLR\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    points = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\\\n",
    "        .mapPartitions(readPointBatch).cache()\n",
    "    iterations = int(sys.argv[2])\n",
    "\n",
    "    # Initialize w to a random value\n",
    "    w = 2 * np.random.ranf(size=D) - 1\n",
    "    print(\"Initial w: \" + str(w))\n",
    "\n",
    "    # Compute logistic regression gradient for a matrix of data points\n",
    "    def gradient(matrix, w):\n",
    "        Y = matrix[:, 0]    # point labels (first column of input file)\n",
    "        X = matrix[:, 1:]   # point coordinates\n",
    "        # For each point (x, y), compute gradient function, then sum these up\n",
    "        return ((1.0 / (1.0 + np.exp(-Y * X.dot(w))) - 1.0) * Y * X.T).sum(1)\n",
    "\n",
    "    def add(x, y):\n",
    "        x += y\n",
    "        return x\n",
    "\n",
    "    for i in range(iterations):\n",
    "        print(\"On iteration %i\" % (i + 1))\n",
    "        w -= points.map(lambda m: gradient(m, w)).reduce(add)\n",
    "\n",
    "    print(\"Final w: \" + str(w))\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is an example implementation of PageRank. For more conventional use,\n",
    "Please refer to PageRank implementation provided by graphx\n",
    "\n",
    "Example Usage:\n",
    "bin/spark-submit examples/src/main/python/pagerank.py data/mllib/pagerank_data.txt 10\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import re\n",
    "import sys\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def computeContribs(urls, rank):\n",
    "    \"\"\"Calculates URL contributions to the rank of other URLs.\"\"\"\n",
    "    num_urls = len(urls)\n",
    "    for url in urls:\n",
    "        yield (url, rank / num_urls)\n",
    "\n",
    "\n",
    "def parseNeighbors(urls):\n",
    "    \"\"\"Parses a urls pair string into urls pair.\"\"\"\n",
    "    parts = re.split(r'\\s+', urls)\n",
    "    return parts[0], parts[1]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 3:\n",
    "        print(\"Usage: pagerank <file> <iterations>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    print(\"WARN: This is a naive implementation of PageRank and is given as an example!\\n\" +\n",
    "          \"Please refer to PageRank implementation provided by graphx\",\n",
    "          file=sys.stderr)\n",
    "\n",
    "    # Initialize the spark context.\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonPageRank\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Loads in input file. It should be in format of:\n",
    "    #     URL         neighbor URL\n",
    "    #     URL         neighbor URL\n",
    "    #     URL         neighbor URL\n",
    "    #     ...\n",
    "    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
    "\n",
    "    # Loads all URLs from input file and initialize their neighbors.\n",
    "    links = lines.map(lambda urls: parseNeighbors(urls)).distinct().groupByKey().cache()\n",
    "\n",
    "    # Loads all URLs with other URL(s) link to from input file and initialize ranks of them to one.\n",
    "    ranks = links.map(lambda url_neighbors: (url_neighbors[0], 1.0))\n",
    "\n",
    "    # Calculates and updates URL ranks continuously using PageRank algorithm.\n",
    "    for iteration in range(int(sys.argv[2])):\n",
    "        # Calculates URL contributions to the rank of other URLs.\n",
    "        contribs = links.join(ranks).flatMap(\n",
    "            lambda url_urls_rank: computeContribs(url_urls_rank[1][0], url_urls_rank[1][1]))\n",
    "\n",
    "        # Re-calculates URL ranks based on neighbor contributions.\n",
    "        ranks = contribs.reduceByKey(add).mapValues(lambda rank: rank * 0.85 + 0.15)\n",
    "\n",
    "    # Collects all URL ranks and dump them to console.\n",
    "    for (link, rank) in ranks.collect():\n",
    "        print(\"%s has rank: %s.\" % (link, rank))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read data file users.parquet in local Spark distro:\n",
    "$ cd $SPARK_HOME\n",
    "$ export AVRO_PARQUET_JARS=/path/to/parquet-avro-1.5.0.jar\n",
    "$ ./bin/spark-submit --driver-class-path /path/to/example/jar \\\\\n",
    "        --jars $AVRO_PARQUET_JARS \\\\\n",
    "        ./examples/src/main/python/parquet_inputformat.py \\\\\n",
    "        examples/src/main/resources/users.parquet\n",
    "<...lots of log output...>\n",
    "{u'favorite_color': None, u'name': u'Alyssa', u'favorite_numbers': [3, 9, 15, 20]}\n",
    "{u'favorite_color': u'red', u'name': u'Ben', u'favorite_numbers': []}\n",
    "<...more log output...>\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"\"\"\n",
    "        Usage: parquet_inputformat.py <data_file>\n",
    "        Run with example jar:\n",
    "        ./bin/spark-submit --driver-class-path /path/to/example/jar \\\\\n",
    "                /path/to/examples/parquet_inputformat.py <data_file>\n",
    "        Assumes you have Parquet data stored in <data_file>.\n",
    "        \"\"\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    path = sys.argv[1]\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"ParquetInputFormat\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    parquet_rdd = sc.newAPIHadoopFile(\n",
    "        path,\n",
    "        'org.apache.parquet.avro.AvroParquetInputFormat',\n",
    "        'java.lang.Void',\n",
    "        'org.apache.avro.generic.IndexedRecord',\n",
    "        valueConverter='org.apache.spark.examples.pythonconverters.IndexedRecordToJavaConverter')\n",
    "    output = parquet_rdd.map(lambda x: x[1]).collect()\n",
    "    for k in output:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PYpi\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "from random import random\n",
    "from operator import add\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "        Usage: pi [partitions]\n",
    "    \"\"\"\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonPi\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    partitions = int(sys.argv[1]) if len(sys.argv) > 1 else 2\n",
    "    n = 100000 * partitions\n",
    "\n",
    "    def f(_):\n",
    "        x = random() * 2 - 1\n",
    "        y = random() * 2 - 1\n",
    "        return 1 if x ** 2 + y ** 2 <= 1 else 0\n",
    "\n",
    "    count = spark.sparkContext.parallelize(range(1, n + 1), partitions).map(f).reduce(add)\n",
    "    print(\"Pi is roughly %f\" % (4.0 * count / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SORT\n",
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: sort <file>\", file=sys.stderr)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .appName(\"PythonSort\")\\\n",
    "        .getOrCreate()\n",
    "\n",
    "    lines = spark.read.text(sys.argv[1]).rdd.map(lambda r: r[0])\n",
    "    sortedCount = lines.flatMap(lambda x: x.split(' ')) \\\n",
    "        .map(lambda x: (int(x), 1)) \\\n",
    "        .sortByKey()\n",
    "    # This is just a demo on how to bring all the sorted data back to a single node.\n",
    "    # In reality, we wouldn't want to collect all the data to the driver node.\n",
    "    output = sortedCount.collect()\n",
    "    for (num, unitcount) in output:\n",
    "        print(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Status API Demo\n",
    "from __future__ import print_function\n",
    "\n",
    "import time\n",
    "import threading\n",
    "import sys\n",
    "if sys.version >= '3':\n",
    "    import queue as Queue\n",
    "else:\n",
    "    import Queue\n",
    "\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "\n",
    "def delayed(seconds):\n",
    "    def f(x):\n",
    "        time.sleep(seconds)\n",
    "        return x\n",
    "    return f\n",
    "\n",
    "\n",
    "def call_in_background(f, *args):\n",
    "    result = Queue.Queue(1)\n",
    "    t = threading.Thread(target=lambda: result.put(f(*args)))\n",
    "    t.daemon = True\n",
    "    t.start()\n",
    "    return result\n",
    "\n",
    "\n",
    "def main():\n",
    "    conf = SparkConf().set(\"spark.ui.showConsoleProgress\", \"false\")\n",
    "    sc = SparkContext(appName=\"PythonStatusAPIDemo\", conf=conf)\n",
    "\n",
    "    def run():\n",
    "        rdd = sc.parallelize(range(10), 10).map(delayed(2))\n",
    "        reduced = rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "        return reduced.map(delayed(2)).collect()\n",
    "\n",
    "    result = call_in_background(run)\n",
    "    status = sc.statusTracker()\n",
    "    while result.empty():\n",
    "        ids = status.getJobIdsForGroup()\n",
    "        for id in ids:\n",
    "            job = status.getJobInfo(id)\n",
    "            print(\"Job\", id, \"status: \", job.status)\n",
    "            for sid in job.stageIds:\n",
    "                info = status.getStageInfo(sid)\n",
    "                if info:\n",
    "                    print(\"Stage %d: %d tasks total (%d active, %d complete)\" %\n",
    "                          (sid, info.numTasks, info.numActiveTasks, info.numCompletedTasks))\n",
    "        time.sleep(1)\n",
    "\n",
    "    print(\"Job results are:\", result.get())\n",
    "    sc.stop()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
