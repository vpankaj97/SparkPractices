{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ITVERSITY CCA-175 Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For LOCAL MACHINE SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<SparkContext master=local appName=ITVERSITY>",
      "text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://DESKTOP-BKFQ2PR:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>local</code></dd>\n              <dt>AppName</dt>\n                <dd><code>ITVERSITY</code></dd>\n            </dl>\n        </div>\n        "
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import findspark as fs\n",
    "import os\n",
    "fs.init()\n",
    "fs.find()\n",
    "\n",
    "data_path=os.path.dirname(\"D://Bigdata_Tuts//data//retail_db//\")\n",
    "data_path_json=os.path.dirname(\"D://Bigdata_Tuts//data//retail_db_json//\")\n",
    "\n",
    "from pyspark.sql import SparkSession,SQLContext,HiveContext\n",
    "spark=SparkSession.builder.appName('ITVERSITY').master('local').getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "sqlcontext=SQLContext(sc)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For VAGRANT SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<SparkContext master=yarn appName=Big_DATA>",
      "text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"http://192.168.1.102:4040\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.4.5</code></dd>\n              <dt>Master</dt>\n                <dd><code>yarn</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Big_DATA</code></dd>\n            </dl>\n        </div>\n        "
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import findspark as fs\n",
    "import os\n",
    "fs.init()\n",
    "fs.find()\n",
    "from pyspark.sql import SparkSession,SQLContext,HiveContext\n",
    "spark=SparkSession.builder.appName('Big_DATA').master('yarn').getOrCreate()\n",
    "sc=spark.sparkContext\n",
    "sqlcontext=SQLContext(sc)\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "+------------+\n|databaseName|\n+------------+\n|     default|\n+------------+\n\n"
    }
   ],
   "source": [
    "df=spark.sql(\"show databases\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center><img align=left src=\"Share/files/images/db.png\" width=\"auto\" height=500 /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's LOAD some DATA\n",
    "### LOCAL FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "D://Bigdata Tutorials//data//retail_db\\order_items MapPartitionsRDD[5] at textFile at <unknown>:0"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "orderItems=sc.textFile(os.path.join(data_path,\"order_items\"))\n",
    "orders=sc.textFile(os.path.join(data_path,\"orders\"))\n",
    "orderItems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For VM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderItems=sc.textFile(\"/public/retail_db/order_items\")\n",
    "orders=sc.textFile(\"/public/retail_db/orders\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the same for both systems\n",
    "### Let's find Some stuff out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "IllegalArgumentException",
     "evalue": "'Pathname /D:/Bigdata Tutorials/data/retail_db/order_items from hdfs://localhost:9000/D:/Bigdata Tutorials/data/retail_db/order_items is not a valid DFS filename.'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mD:\\Windows\\Documents\\pySpark\\Spark-2.4.5\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Windows\\Documents\\pySpark\\Spark-2.4.5\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o41.partitions.\n: java.lang.IllegalArgumentException: Pathname /D:/Bigdata Tutorials/data/retail_db/order_items from hdfs://localhost:9000/D:/Bigdata Tutorials/data/retail_db/order_items is not a valid DFS filename.\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getPathName(DistributedFileSystem.java:197)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$000(DistributedFileSystem.java:106)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1305)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem$22.doCall(DistributedFileSystem.java:1301)\r\n\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\r\n\tat org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1317)\r\n\tat org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:57)\r\n\tat org.apache.hadoop.fs.Globber.glob(Globber.java:252)\r\n\tat org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:1676)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:259)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:204)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:273)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:269)\r\n\tat scala.Option.getOrElse(Option.scala:121)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:269)\r\n\tat org.apache.spark.api.java.JavaRDDLike$class.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7331dd385750>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# It contains OrderID\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;33m(\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morderItems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morderItems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\Windows\\Documents\\pySpark\\Spark-2.4.5\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m         \"\"\"\n\u001b[1;32m-> 1378\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1379\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Windows\\Documents\\pySpark\\Spark-2.4.5\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \"\"\"\n\u001b[0;32m   1326\u001b[0m         \u001b[0mitems\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m         \u001b[0mtotalParts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Windows\\Documents\\pySpark\\Spark-2.4.5\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \"\"\"\n\u001b[1;32m--> 391\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Windows\\Documents\\pySpark\\Spark-2.4.5\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Windows\\Documents\\pySpark\\Spark-2.4.5\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: 'Pathname /D:/Bigdata Tutorials/data/retail_db/order_items from hdfs://localhost:9000/D:/Bigdata Tutorials/data/retail_db/order_items is not a valid DFS filename.'"
     ]
    }
   ],
   "source": [
    "# It contains OrderID\n",
    "(int(orderItems.first().split(\",\")[1]),float(orderItems.first().split(\",\")[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderItemsMap=orderItems.map(lambda x:(int(x.split(\",\")[1]),float(x.split(\",\")[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(1, 299.98), (2, 199.99), (2, 250.0), (2, 129.99), (4, 49.98)]"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "orderItemsMap.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can hold 1st element as KEY and reduce the Values using add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1, 299.98)\n(2, 579.98)\n(4, 699.85)\n(5, 1129.8600000000001)\n(7, 579.9200000000001)\n(8, 729.8399999999999)\n(9, 599.96)\n(10, 651.9200000000001)\n(11, 919.79)\n(12, 1299.8700000000001)\n"
    }
   ],
   "source": [
    "from operator import add\n",
    "revenuePerOrder=orderItemsMap.reduceByKey(add)\n",
    "for i in revenuePerOrder.take(10): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformations follow LazyEvaluation.\n",
    "\n",
    "#### LazyEvaluation simply uses a DAG(Directed Acyclic Graph) to store all the information related to the Transformations being made.\n",
    "\n",
    "#### As soon as an Action is 'run', spark executes the DAG first and then the action.\n",
    "\n",
    "Let's find DAG of transformations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'(1) D://Bigdata Tutorials//data//retail_db\\\\order_items MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\\n |  D://Bigdata Tutorials//data//retail_db\\\\order_items HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []'"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "orderItems.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'(1) PythonRDD[12] at RDD at PythonRDD.scala:53 []\\n |  D://Bigdata Tutorials//data//retail_db\\\\order_items MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\\n |  D://Bigdata Tutorials//data//retail_db\\\\order_items HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []'"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "orderItemsMap.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'(1) PythonRDD[13] at RDD at PythonRDD.scala:53 []\\n |  MapPartitionsRDD[10] at mapPartitions at PythonRDD.scala:133 []\\n |  ShuffledRDD[9] at partitionBy at NativeMethodAccessorImpl.java:0 []\\n +-(1) PairwiseRDD[8] at reduceByKey at <ipython-input-6-409d4c32b590>:2 []\\n    |  PythonRDD[7] at reduceByKey at <ipython-input-6-409d4c32b590>:2 []\\n    |  D://Bigdata Tutorials//data//retail_db\\\\order_items MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0 []\\n    |  D://Bigdata Tutorials//data//retail_db\\\\order_items HadoopRDD[0] at textFile at NativeMethodAccessorImpl.java:0 []'"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "revenuePerOrder.toDebugString()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we can open spark job UI and look into DAG visualization.\n",
    "\n",
    "\n",
    "### NOTE: DO NOT USE collect() to preview data in REAL LIFE ENVIRONMENT!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way tp create an RDD is to open a file using open and making a LIST out of it. (a collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<type 'list'>\n<class 'pyspark.rdd.RDD'>\n1,2,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U,,59.98,http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy\n<type 'str'>\n"
    }
   ],
   "source": [
    "productsRaw=open(data_path+\"//products\").read().splitlines()\n",
    "print(type(productsRaw))\n",
    "### Now, we can create an RDD using Parallelize from the collection.\n",
    "productsRaw=sc.parallelize(productsRaw)\n",
    "print(type(productsRaw))\n",
    "print(productsRaw.first())\n",
    "print(type(productsRaw.first()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA FRAME\n",
    "\n",
    "#### --provided by sqlContext\n",
    "\n",
    "#### Now we can try loading multiple file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- order_item_id: long (nullable = true)\n |-- order_item_order_id: long (nullable = true)\n |-- order_item_product_id: long (nullable = true)\n |-- order_item_product_price: double (nullable = true)\n |-- order_item_quantity: long (nullable = true)\n |-- order_item_subtotal: double (nullable = true)\n\n"
    }
   ],
   "source": [
    "df = spark.read.json(data_path_json+\"//order_items\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "root\n |-- order_customer_id: long (nullable = true)\n |-- order_date: string (nullable = true)\n |-- order_id: long (nullable = true)\n |-- order_status: string (nullable = true)\n\n"
    }
   ],
   "source": [
    "df1=spark.read.format(\"json\").load(data_path_json+\"//orders\")\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's continue with the Orders and OrderItems Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "### 1. Extract OrderStatus: (YYYYMMDD,count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(u'20130725', u'CLOSED')"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "ordersMap=orders.map(lambda x: (x.split(\",\")[1].split(\" \")[0].replace(\"-\",\"\"),x.split(\",\")[3]))\n",
    "ordersMap.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Filtering CLOSED/COMPLETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[u'1,2013-07-25 00:00:00.0,11599,CLOSED',\n u'3,2013-07-25 00:00:00.0,12111,COMPLETE',\n u'4,2013-07-25 00:00:00.0,8827,CLOSED',\n u'5,2013-07-25 00:00:00.0,11318,COMPLETE',\n u'6,2013-07-25 00:00:00.0,7130,COMPLETE']"
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "# To get data of COMPLETE ORDERS OR CLOSED orders\n",
    "ordersComplete=orders.filter(lambda x: x.split(\",\")[3] == 'COMPLETE' or x.split(\",\")[3] == 'CLOSED')\n",
    "ordersComplete.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Filtering CLOSED/COMPLETE in 2014-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[u'25882,2014-01-01 00:00:00.0,4598,COMPLETE',\n u'25888,2014-01-01 00:00:00.0,6735,COMPLETE',\n u'25889,2014-01-01 00:00:00.0,10045,COMPLETE',\n u'25891,2014-01-01 00:00:00.0,3037,CLOSED',\n u'25895,2014-01-01 00:00:00.0,1044,COMPLETE']"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "#To get CLOSED/COMPLETE orders in 2014-01\n",
    "ordersComplete=orders.filter(lambda x: (x.split(\",\")[1][:7]=='2014-01') and (x.split(\",\")[3] in ['COMPLETE','CLOSED'] ))\n",
    "ordersComplete.take(5)\n",
    "\n",
    "#Note: (x.split(\",\")[3] in ['COMPLETE','CLOSED']) is same as (x.split(\",\")[3] == 'COMPLETE' or x.split(\",\")[3] == 'CLOSED')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Join datasets where C{self} has (k,v) and C{other} has (k,w)\n",
    "#### Note: After join --> (k,(v,w))\n",
    "#### Thus, we need two tables; Parent & Child Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We know that, \n",
    "* \"orders\" has 1st element as order_id\n",
    "* \"orderItems\" has 2nd element as FK to order_id\n",
    "### We need to convert RDDs to (k,v) format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(1, 'CLOSED'),\n (2, 'PENDING_PAYMENT'),\n (3, 'COMPLETE'),\n (4, 'CLOSED'),\n (5, 'COMPLETE')]"
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "source": [
    "#Converting orders RDD to (k,v)\n",
    "ordersMapJoin=orders.map(lambda x: (int(x.split(\",\")[0]),str(x.split(\",\")[3])))\n",
    "ordersMapJoin.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(1, (957, 299.98, 1, 299.98)),\n (2, (1073, 199.99, 1, 199.99)),\n (2, (502, 50.0, 5, 250.0)),\n (2, (403, 129.99, 1, 129.99)),\n (4, (897, 24.99, 2, 49.98))]"
     },
     "metadata": {},
     "execution_count": 100
    }
   ],
   "source": [
    "#Converting orderItems RDD to (k,w)\n",
    "# int(x.split(\",\")[1]) - Order ID\n",
    "#int(x.split(\",\")[2]) - Product ID \n",
    "#float(x.split(\",\")[5]) - Product - Price\n",
    "#int(x.split(\",\")[3]) - Product Count\n",
    "#float(x.split(\",\")[4]) - Subtotal \n",
    "\n",
    "orderItemsMapJoin=orderItems.map(lambda x: (int(x.split(\",\")[1]),(int(x.split(\",\")[2]),float(x.split(\",\")[5]),int(x.split(\",\")[3]),float(x.split(\",\")[4]))))\n",
    "orderItemsMapJoin.take(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, we can join these 2 RDDs to get information about ORDER_ID with details relating to the orderItems present in that order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(2, ('PENDING_PAYMENT', (1073, 199.99, 1, 199.99)))\n(2, ('PENDING_PAYMENT', (502, 50.0, 5, 250.0)))\n(2, ('PENDING_PAYMENT', (403, 129.99, 1, 129.99)))\n(4, ('CLOSED', (897, 24.99, 2, 49.98)))\n(4, ('CLOSED', (365, 59.99, 5, 299.95)))\n(4, ('CLOSED', (502, 50.0, 3, 150.0)))\n(4, ('CLOSED', (1014, 49.98, 4, 199.92)))\n(8, ('PROCESSING', (365, 59.99, 3, 179.97)))\n(8, ('PROCESSING', (365, 59.99, 5, 299.95)))\n(8, ('PROCESSING', (1014, 49.98, 4, 199.92)))\n"
    }
   ],
   "source": [
    "ordersJoin=ordersMapJoin.join(orderItemsMapJoin)\n",
    "for i in ordersJoin.take(10): print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Let's Try to figure out the STATUS and Revenue on on a particular order:\n",
    "\n",
    "##### (Say, To get details of an order) ex. Order 2 contains 3 items - 1073,502,403"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(2,\n ('PENDING_PAYMENT',\n  (1073, 199.99, 1, 199.99),\n  'PENDING_PAYMENT',\n  (502, 50.0, 5, 250.0),\n  'PENDING_PAYMENT',\n  (403, 129.99, 1, 129.99)))"
     },
     "metadata": {},
     "execution_count": 78
    }
   ],
   "source": [
    "ordersJoin.reduceByKey(add).first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now trying to get total revenue from the previous RDD, we can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(2, 579.98),\n (4, 699.85),\n (8, 729.8399999999999),\n (10, 651.9200000000001),\n (12, 1299.8700000000001)]"
     },
     "metadata": {},
     "execution_count": 83
    }
   ],
   "source": [
    "ordersJoin.map(lambda x:(x[0],x[1][1][3])).reduceByKey(add).take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordersJoin.map(lambda x:(x[0],x[1][1][3])).reduce(add).take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Order is successful but no corresponding entry in orderItems - TROUBLESHOOT PROBLEM\n",
    "#### We can use outer joins:\n",
    "\n",
    "* leftOuterJoin: Keep parent table on the left.\n",
    "    * Followed by filter\n",
    "\n",
    "* rightOuterJoin: Keep the parent table on the right.\n",
    "    * Followed by map\n",
    "\n",
    "* fullOuterJoin: When both tables have some unique values that need to be joined together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(6, ('COMPLETE', None)),\n (22, ('COMPLETE', None)),\n (26, ('COMPLETE', None)),\n (32, ('COMPLETE', None)),\n (40, ('PENDING_PAYMENT', None))]"
     },
     "metadata": {},
     "execution_count": 108
    }
   ],
   "source": [
    "ordersLOJoin=ordersMapJoin.leftOuterJoin(orderItemsMapJoin)\n",
    "ordersLOJoin.filter(lambda x: x[1][1]==None).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(u'20140119', (None, u'CANCELED')),\n (u'20140119', (None, u'PROCESSING')),\n (u'20140119', (None, u'PENDING_PAYMENT')),\n (u'20140119', (None, u'COMPLETE')),\n (u'20140119', (None, u'CANCELED'))]"
     },
     "metadata": {},
     "execution_count": 111
    }
   ],
   "source": [
    "# We can do the same thing with right outer Join\n",
    "ordersROJoin=orderItemsMap.rightOuterJoin(ordersMap)\n",
    "ordersROJoin.filter(lambda x: x[1][0]==None).take(5)\n",
    "\n",
    "##THIS STILL GIVES US DATA about a complete order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(2, (None, 199.99)),\n (2, (None, 250.0)),\n (2, (None, 129.99)),\n (4, (None, 49.98)),\n (4, (None, 299.95))]"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "# Note: if we had NOT placed the parent folder on the right:\n",
    "ordersROJoin=ordersMap.rightOuterJoin(orderItemsMap)\n",
    "ordersROJoin.filter(lambda x: x[1][0]==None).take(5)\n",
    "\n",
    "#THIS WON'T HELP US!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Find the orderItem with minimum value(cost) in a order\n",
    "\n",
    "#### We will need to:\n",
    "\n",
    "* Filter the RDD to show a particular order - O/P another RDD\n",
    "* reduce the RDD to get the minimum value orderItem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "u'5,4,897,2,49.98,24.99'"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "orderItems.filter(lambda x: int(x.split(\",\")[1])==4).reduce(lambda x,y: x if(float(x.split(\",\")[4]) < float(y.split(\",\")[4])) else y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Find the orderItem with minimum value(cost) in all orders\n",
    "\n",
    "#### We will need to:\n",
    "\n",
    "* Map the RDD to (k,v) order - O/P another RDD\n",
    "* reduceByKey the RDD to get the minimum value of all orders. (using the same logic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(1, u'1,1,957,1,299.98,299.98'),\n (2, u'4,2,403,1,129.99,129.99'),\n (4, u'5,4,897,2,49.98,24.99'),\n (5, u'11,5,1014,2,99.96,49.98'),\n (7, u'16,7,926,5,79.95,15.99')]"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "##to generalize: WE CAN USE reduceByKey\n",
    "orderItems.map(lambda x: (int(x.split(\",\")[1]),x)).reduceByKey(lambda x,y: x if(float(x.split(\",\")[4]) < float(y.split(\",\")[4])) else y).take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Group the cost of each item and finally get TotalRevenue per order.\n",
    "\n",
    "#### We can use groupByKey() followed by map - (Instead of better option - reduceByKey())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(1, 299.98),\n (2, 579.98),\n (4, 699.85),\n (5, 1129.8600000000001),\n (7, 579.9200000000001)]"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "orderItemsMap.groupByKey().map(lambda x: (x[0],sum(x[1]))).take(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Group orders based of revenue of orderItem in DESC (sort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "(1, [299.98])\n(2, [250.0, 199.99, 129.99])\n(4, [299.95, 199.92, 150.0, 49.98])\n(5, [299.98, 299.98, 299.95, 129.99, 99.96])\n(7, [299.98, 199.99, 79.95])\n"
    }
   ],
   "source": [
    "# THIS IS FOR VIEW ONLY\n",
    "\n",
    "for k,v in orderItemsMap.groupByKey().take(5):\n",
    "    print(k,sorted(list(v),reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[(1, [299.98]),\n (2, [129.99, 199.99, 250.0]),\n (4, [49.98, 150.0, 199.92, 299.95]),\n (5, [99.96, 129.99, 299.95, 299.98, 299.98]),\n (7, [79.95, 199.99, 299.98])]"
     },
     "metadata": {},
     "execution_count": 71
    }
   ],
   "source": [
    "# WE WILL DO THIS IN PRODUCTION\n",
    "\n",
    "##In general\n",
    "orderItemsMap.groupByKey(). \\\n",
    "map(lambda x: (x[0],sorted(list(x[1])))). \\\n",
    "take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[1, [299.98], 2, [129.99, 199.99, 250.0], 4]"
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "#OR\n",
    "\n",
    "## If we need individual values:\n",
    "\n",
    "orderItemsMap.groupByKey(). \\\n",
    "flatMap(lambda x: (x[0],sorted(list(x[1])))). \\\n",
    "take(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 32-bit",
   "language": "python",
   "name": "python37332bit4a292faee69342a689a990d8953bfb01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}