{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For Local Machine\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating SPARK session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession. \\\n",
    "builder. \\\n",
    "appName('Testing'). \\\n",
    "master('local'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Testing</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=Testing>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.app.id', 'local-1589264695139'),\n",
       " ('spark.driver.extraJavaOptions', '-Dderby.system.home=/tmp/derby'),\n",
       " ('spark.app.name', 'Testing'),\n",
       " ('spark.sql.repl.eagerEval.enabled', 'true'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '37388'),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.driver.host', '10.0.2.15'),\n",
       " ('spark.ui.showConsoleProgress', 'true')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD=sc.parallelize([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallelize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumSlices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Distribute a local Python collection to form an RDD. Using xrange\n",
       "is recommended if the input represents a range for performance.\n",
       "\n",
       ">>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
       "[[0], [2], [3], [4], [6]]\n",
       ">>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
       "[[], [0], [], [2], [4]]\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/context.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sc.parallelize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Each mnemonic uses blue rectangles to represent elements in the original RDD (resilient distributed dataset).\n",
    "* The left side represents the input RDD and the right side represents the output.\n",
    "* Elements in the output RDD may contain original elements (blue), elements with potentially different values (purple), or elements with potentially different data types (orange).\n",
    "* In some cases, the output is a python object on the driver (dotted rectangle).\n",
    "* When key-value pairs are critical to the operation, the 'key' is represented by a black square in the upper left corner, and the 'value' is represented by the remaining rectangle.\n",
    "* User defined functions are represented by a green rectangle.\n",
    "* When relevant to the core operation, partitions are represented by diagonal lines to the left of the RDD.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a>\n",
    "<img align=left src=\"Share/files/images/pyspark-page2.svg\" width=500 height=500>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/2.4.5/api/python/pyspark.html#pyspark.RDD.map\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page3.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.map?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map1\n",
    "x = sc.parallelize([1,2,3]) # sc = spark context, parallelize creates an RDD from the passed object\n",
    "y = x.map(lambda x: (x,x**2))\n",
    "print(x.collect())  # collect copies RDD elements to a list on the driver\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map2\n",
    "x=sc.parallelize(['b','a','c'])\n",
    "y=sorted(x.map(lambda x:(x,1)).collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.collect?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/2.4.5/api/python/pyspark.html#pyspark.RDD.flatMap\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page4.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.flatMap?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatMap1\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.flatMap(lambda x: (x, 100*x, x**2))\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatMap2\n",
    "x=sc.parallelize([2,3,4])\n",
    "y=x.flatMap(lambda x:range(1,x)).collect()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/2.4.5/api/python/pyspark.html#pyspark.RDD.mapPartitions\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page5.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.mapPartitions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapPartitions1\n",
    "x = sc.parallelize([1,2,3,4], 2)\n",
    "def f(iterator): yield sum(iterator)\n",
    "y = x.mapPartitions(f)\n",
    "print(x.glom().collect())  # glom() flattens elements on the same partition\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.glom?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mapPartitionsWithIndex\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page6.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD.mapPartitionsWithIndex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapPartitionsWithIndex\n",
    "x = sc.parallelize([1,2,3,4], 2)\n",
    "def f(partitionIndex, iterator): yield (partitionIndex,sum(iterator))\n",
    "y = x.mapPartitionsWithIndex(f)\n",
    "print(x.glom().collect())  # glom() flattens elements on the same partition\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.getNumPartitions\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page7.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2], [3, 4]]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# getNumPartitions\n",
    "x = sc.parallelize([1,2,3,4], 3)\n",
    "y = x.getNumPartitions()\n",
    "print(x.glom().collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.filter\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page8.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return a new RDD containing only the elements that satisfy a predicate.\n",
       "\n",
       ">>> rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
       ">>> rdd.filter(lambda x: x % 2 == 0).collect()\n",
       "[2, 4]\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.filter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[1, 3]\n"
     ]
    }
   ],
   "source": [
    "# filter\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.filter(lambda x: x%2 == 1)  # filters out even elements\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.distinct\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page9.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumPartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return a new RDD containing the distinct elements in this RDD.\n",
       "\n",
       ">>> sorted(sc.parallelize([1, 1, 2, 3]).distinct().collect())\n",
       "[1, 2, 3]\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.distinct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'A', 'B']\n",
      "['A', 'B']\n"
     ]
    }
   ],
   "source": [
    "# distinct\n",
    "x = sc.parallelize(['A','A','B'])\n",
    "y = x.distinct()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sample\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page10.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0, 1, 2, 3, 4, 5, 6]\n",
      "sample:0 y = [6]\n",
      "sample:1 y = [1, 4, 5]\n",
      "sample:2 y = [0, 3, 4, 5]\n",
      "sample:3 y = [0, 2, 3, 5]\n",
      "sample:4 y = [1, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "x = sc.parallelize(range(7))\n",
    "ylist = [x.sample(withReplacement=False, fraction=0.5) for i in range(5)] # call 'sample' 5 times\n",
    "print('x = ' + str(x.collect()))\n",
    "for cnt,y in zip(range(len(ylist)), ylist):\n",
    "    print('sample:' + str(cnt) + ' y = ' +  str(y.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.takeSample\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page11.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtakeSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithReplacement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return a fixed-size sampled subset of this RDD.\n",
       "\n",
       ".. note:: This method should only be used if the resulting array is expected\n",
       "    to be small, as all the data is loaded into the driver's memory.\n",
       "\n",
       ">>> rdd = sc.parallelize(range(0, 10))\n",
       ">>> len(rdd.takeSample(True, 20, 1))\n",
       "20\n",
       ">>> len(rdd.takeSample(False, 5, 2))\n",
       "5\n",
       ">>> len(rdd.takeSample(False, 15, 3))\n",
       "10\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.takeSample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = [0, 1, 2, 3, 4, 5, 6]\n",
      "sample:0 y = [3, 0, 1]\n",
      "sample:1 y = [2, 0, 1]\n",
      "sample:2 y = [2, 4, 0]\n",
      "sample:3 y = [2, 3, 1]\n",
      "sample:4 y = [6, 5, 1]\n"
     ]
    }
   ],
   "source": [
    "# takeSample\n",
    "x = sc.parallelize(range(7))\n",
    "ylist = [x.takeSample(withReplacement=False, num=3) for i in range(5)]  # call 'sample' 5 times\n",
    "print('x = ' + str(x.collect()))\n",
    "for cnt,y in zip(range(len(ylist)), ylist):\n",
    "    print('sample:' + str(cnt) + ' y = ' +  str(y))  # no collect on y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.union\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page12.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return the union of this RDD and another one.\n",
       "\n",
       ">>> rdd = sc.parallelize([1, 1, 2, 3])\n",
       ">>> rdd.union(rdd).collect()\n",
       "[1, 1, 2, 3, 1, 1, 2, 3]\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.union?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD>union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'A', 'B', 'D', 'C', 'A']\n"
     ]
    }
   ],
   "source": [
    "# union\n",
    "x = sc.parallelize(['A','A','B'])\n",
    "y = sc.parallelize(['D','C','A'])\n",
    "print(x.union(y).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.intersection\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page13.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintersection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return the intersection of this RDD and another one. The output will\n",
       "not contain any duplicate elements, even if the input RDDs did.\n",
       "\n",
       ".. note:: This method performs a shuffle internally.\n",
       "\n",
       ">>> rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])\n",
       ">>> rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])\n",
       ">>> rdd1.intersection(rdd2).collect()\n",
       "[1, 2, 3]\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.intersection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'A', 'B']\n",
      "['A', 'B', 'D']\n",
      "['A', 'B']\n"
     ]
    }
   ],
   "source": [
    "# intersection\n",
    "x = sc.parallelize(['A','A','B'])\n",
    "y = sc.parallelize(['A','B','D'])\n",
    "z = x.intersection(y).collect()\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sortByKey\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page14.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortByKey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnumPartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkeyfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7f3868036488\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Sorts this RDD, which is assumed to consist of (key, value) pairs.\n",
       "\n",
       ">>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
       ">>> sc.parallelize(tmp).sortByKey().first()\n",
       "('1', 3)\n",
       ">>> sc.parallelize(tmp).sortByKey(True, 1).collect()\n",
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
       ">>> sc.parallelize(tmp).sortByKey(True, 2).collect()\n",
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
       ">>> tmp2 = [('Mary', 1), ('had', 2), ('a', 3), ('little', 4), ('lamb', 5)]\n",
       ">>> tmp2.extend([('whose', 6), ('fleece', 7), ('was', 8), ('white', 9)])\n",
       ">>> sc.parallelize(tmp2).sortByKey(True, 3, keyfunc=lambda k: k.lower()).collect()\n",
       "[('a', 3), ('fleece', 7), ('had', 2), ('lamb', 5),...('white', 9), ('whose', 6)]\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.sortByKey?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', 1), ('a', 2), ('C', 3)]\n",
      "[('1', 1), ('C', 3), ('a', 2)]\n"
     ]
    }
   ],
   "source": [
    "# sortByKey1\n",
    "x = sc.parallelize([('1',1),('a',2),('C',3)])\n",
    "y = x.sortByKey()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprtByKey2\n",
    "tmp=[('Marry',1),('had',2),('a',3),('little',4),('lamb',5)]\n",
    "tmp.extend([('fleece',6)])\n",
    "x=sc.parallelize(tmp)\n",
    "print(x.sortByKey(True,3).collect())\n",
    "print(x.sortByKey(True,3,keyfunc=lambda k: k.lower()).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m L.extend(iterable) -> None -- extend list by appending elements from the iterable\n",
       "\u001b[0;31mType:\u001b[0m      builtin_function_or_method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp.extend?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sortBy\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page15.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msortBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumPartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Sorts this RDD by the given keyfunc\n",
       "\n",
       ">>> tmp = [('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
       ">>> sc.parallelize(tmp).sortBy(lambda x: x[0]).collect()\n",
       "[('1', 3), ('2', 5), ('a', 1), ('b', 2), ('d', 4)]\n",
       ">>> sc.parallelize(tmp).sortBy(lambda x: x[1]).collect()\n",
       "[('a', 1), ('b', 2), ('1', 3), ('d', 4), ('2', 5)]\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.sortBy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'Bat', 'Cat']\n"
     ]
    }
   ],
   "source": [
    "# sortBy\n",
    "x = sc.parallelize(['Cat','Apple','Bat'])\n",
    "y = x.sortBy(lambda k: k[0]).collect()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.glom\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page16.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return an RDD created by coalescing all elements within each partition\n",
       "into a list.\n",
       "\n",
       ">>> rdd = sc.parallelize([1, 2, 3, 4], 2)\n",
       ">>> sorted(rdd.glom().collect())\n",
       "[[1, 2], [3, 4]]\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.glom?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C', 'B', 'A']\n",
      "[['C'], ['B', 'A']]\n"
     ]
    }
   ],
   "source": [
    "# glom\n",
    "x = sc.parallelize(['C','B','A'], 2)\n",
    "y = x.glom()\n",
    "print(x.collect()) \n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.cartesian\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page17.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcartesian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return the Cartesian product of this RDD and another one, that is, the\n",
       "RDD of all pairs of elements C{(a, b)} where C{a} is in C{self} and\n",
       "C{b} is in C{other}.\n",
       "\n",
       ">>> rdd = sc.parallelize([1, 2])\n",
       ">>> sorted(rdd.cartesian(rdd).collect())\n",
       "[(1, 1), (1, 2), (2, 1), (2, 2)]\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.cartesian?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'B']\n",
      "['C', 'D']\n",
      "[('A', 'C'), ('A', 'D'), ('B', 'C'), ('B', 'D')]\n"
     ]
    }
   ],
   "source": [
    "# cartesian\n",
    "x = sc.parallelize(['A','B'])\n",
    "y = sc.parallelize(['C','D'])\n",
    "z = x.cartesian(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupBy\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page18.svg\" width=500 height=500 />\n",
    "<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnumPartitions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpartitionFunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0mfunction\u001b[0m \u001b[0mportable_hash\u001b[0m \u001b[0mat\u001b[0m \u001b[0;36m0x7f3870378ea0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return an RDD of grouped items.\n",
       "\n",
       ">>> rdd = sc.parallelize([1, 1, 2, 3, 5, 8])\n",
       ">>> result = rdd.groupBy(lambda x: x % 2).collect()\n",
       ">>> sorted([(x, sorted(y)) for (x, y) in result])\n",
       "[(0, [2, 8]), (1, [1, 1, 3, 5])]\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.groupBy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('A', [2, 8]), ('B', [1, 1, 3, 5])]\n"
     ]
    }
   ],
   "source": [
    "x=sc.parallelize([1,1,2,3,5,8])\n",
    "result=x.groupBy(lambda x: 'A' if x%2==0 else 'B').collect()\n",
    "print(sorted([(x, sorted(y)) for (x, y) in result]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "[('B', [1, 3]), ('A', [2])]\n",
      "[('A', [2]), ('B', [1, 3])]\n"
     ]
    }
   ],
   "source": [
    "# groupBy\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.groupBy(lambda x: 'A' if (x%2 == 0) else 'B' ).collect()\n",
    "print(x.collect())\n",
    "print([(j[0],[i for i in j[1]]) for j in y])# y is nested, this iterates through it\n",
    "#different way\n",
    "print(sorted([(x,sorted(y)) for (x,y) in y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.pipe\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page19.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckCode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return an RDD created by piping elements to a forked external process.\n",
       "\n",
       ">>> sc.parallelize(['1', '2', '', '3']).pipe('cat').collect()\n",
       "['1', '2', '', '3']\n",
       "\n",
       ":param checkCode: whether or not to check the return value of the shell command.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.pipe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3']\n"
     ]
    }
   ],
   "source": [
    "#Linux\n",
    "x=sc.parallelize(['1','2','','3'])\n",
    "y=x.pipe('grep \"3\"')\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actions & Transformations difference\n",
    "Map is a transformation, thus when you perform a map you apply a function to each element in the RDD and return a new RDD where additional transformations or actions can be called.\n",
    "\n",
    "Foreach is an action, it takes each element and applies a function, but it does not return a value. This is particularly useful in you have to call perform some calculation on an RDD and log the result somewhere else, for example a database or call a REST API with each element in the RDD.\n",
    "\n",
    "For example let's say that you have an RDD with many queries that you wish to log in another system. The queries are stored in an RDD."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "queries = <code to load queries or a transformation that was applied on other RDDs>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you want to save those queries in another system via a call to another "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import urllib2\n",
    "\n",
    "def log_search(q):\n",
    "    response = urllib2.urlopen('http://www.bigdatainc.org/save_query/' + q)\n",
    "\n",
    "queries.foreach(call_search)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have executed the log_query on each element of the RDD. If you have done a map, nothing would have happened yet, until you called an action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foreach\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page20.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Applies a function to all elements of this RDD.\n",
       "\n",
       ">>> def f(x): print(x)\n",
       ">>> sc.parallelize([1, 2, 3, 4, 5]).foreach(f)\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.foreach?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 2, 1]\n",
      "None\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# foreach action best example\n",
    "from __future__ import print_function\n",
    "x = sc.parallelize([1,2,3,2,1])\n",
    "def f(el): \n",
    "    '''side effect: append the current RDD elements to a file'''\n",
    "    f1=open(\"Share/foreachExample.txt\", 'a+') \n",
    "    print(el,file=f1)\n",
    "\n",
    "open('Share/foreachExample.txt', 'w').close()  # first clear the file contents\n",
    "\n",
    "y = x.foreach(f) # writes into foreachExample.txt\n",
    "\n",
    "print(x.collect())\n",
    "print(y) # foreach returns 'None'\n",
    "# print the contents of foreachExample.txt\n",
    "f1=open(\"Share/foreachExample.txt\", 'r')\n",
    "print(f1.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5]]\n"
     ]
    }
   ],
   "source": [
    "#again this is bullshit\n",
    "def f(x):\n",
    "    f=open(\"Share/exp1.txt\",'a+')\n",
    "    print([x],file=f)\n",
    "x=sc.parallelize([1,2,3,4,5])\n",
    "x.foreach(f)\n",
    "print(x.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foreachPartition\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page21.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeachPartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Applies a function to each partition of this RDD.\n",
       "\n",
       ">>> def f(iterator):\n",
       "...     for x in iterator:\n",
       "...          print(x)\n",
       ">>> sc.parallelize([1, 2, 3, 4, 5]).foreachPartition(f)\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.foreachPartition?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [4], [5, 6], [7, 8]]\n",
      "None\n",
      "[1]\n",
      "[2, 3]\n",
      "[4]\n",
      "[5, 6]\n",
      "[7, 8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# foreachPartition\n",
    "#from __future__ import print_function\n",
    "x = sc.parallelize([1,2,3,4,5,6,7,8],5)\n",
    "def f(parition):\n",
    "    '''side effect: append the current RDD partition contents to a file'''\n",
    "    f1=open(\"Share/foreachPartitionExample.txt\", 'a+') \n",
    "    print([el for el in parition],file=f1)\n",
    "\n",
    "open('Share/foreachPartitionExample.txt', 'w').close()  # first clear the file contents\n",
    "\n",
    "y = x.foreachPartition(f) # writes into foreachExample.txt\n",
    "\n",
    "print(x.glom().collect())\n",
    "print(y)  # foreach returns 'None'\n",
    "# print the contents of foreachExample.txt\n",
    "f1 = open(\"Share/foreachPartitionExample.txt\", \"r\")\n",
    "print (f1.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1], [2, 3], [4, 5]]\n"
     ]
    }
   ],
   "source": [
    "#Not BUllshit anywhere\n",
    "def f(x):\n",
    "    f=open(\"Share/exp.txt\",'a+')\n",
    "    print([i for i in x],file=f)\n",
    "x=sc.parallelize([1,2,3,4,5],3)\n",
    "x.foreachPartition(f)\n",
    "print(x.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.collect\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page22.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return a list that contains all of the elements in this RDD.\n",
       "\n",
       ".. note:: This method should only be used if the resulting array is expected\n",
       "    to be small, as all the data is loaded into the driver's memory.\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.collect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[169] at parallelize at PythonRDD.scala:195 [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "# collect\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.collect()\n",
    "print(x,y)  # distributed # not distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduce\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page23.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Reduces the elements of this RDD using the specified commutative and\n",
       "associative binary operator. Currently reduces partitions locally.\n",
       "\n",
       ">>> from operator import add\n",
       ">>> sc.parallelize([1, 2, 3, 4, 5]).reduce(add)\n",
       "15\n",
       ">>> sc.parallelize((2 for _ in range(10))).map(lambda x: 1).cache().reduce(add)\n",
       "10\n",
       ">>> sc.parallelize([]).reduce(add)\n",
       "Traceback (most recent call last):\n",
       "    ...\n",
       "ValueError: Can not reduce() empty RDD\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.reduce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "print(sc.parallelize([1,2,3,4,5]).reduce(add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(sc.parallelize((6 for i in range(10))).map(lambda x:1).reduce(add))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# reduce\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.reduce(add)  # computes a cumulative sum\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.fold\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page24.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzeroValue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Aggregate the elements of each partition, and then the results for all\n",
       "the partitions, using a given associative function and a neutral \"zero value.\"\n",
       "\n",
       "The function C{op(t1, t2)} is allowed to modify C{t1} and return it\n",
       "as its result value to avoid object allocation; however, it should not\n",
       "modify C{t2}.\n",
       "\n",
       "This behaves somewhat differently from fold operations implemented\n",
       "for non-distributed collections in functional languages like Scala.\n",
       "This fold operation may be applied to partitions individually, and then\n",
       "fold those results into the final result, rather than apply the fold\n",
       "to each element sequentially in some defined ordering. For functions\n",
       "that are not commutative, the result may differ from that of a fold\n",
       "applied to a non-distributed collection.\n",
       "\n",
       ">>> from operator import add\n",
       ">>> sc.parallelize([1, 2, 3, 4, 5]).fold(0, add)\n",
       "15\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.fold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "# fold\n",
    "x = sc.parallelize([1,2,3])\n",
    "neutral_zero_value = 1  # 0 for sum, 1 for multiplication\n",
    "y = x.fold(neutral_zero_value,lambda obj, accumulated: accumulated * obj) # computes cumulative sum\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.aggregate\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page25.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aggregate() lets you take an RDD and generate a single value that is of a different type than what was stored in the original RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zeroValue: The initialization value, for your result, in the desired format.\n",
    "seqOp: The operation you want to apply to RDD records. Runs once for every record in a partition.\n",
    "combOp: Defines how the resulted objects (one for every partition), gets combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15, 5)\n"
     ]
    }
   ],
   "source": [
    "seqOp=(lambda x,y:(x[0]+y,x[1]+1))\n",
    "combOp=(lambda x,y:(x[0]+y[0],x[1]+y[1]))\n",
    "y=sc.parallelize([1,2,3,4,5],2).aggregate((0,0),seqOp,combOp)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "          (0, 0) <-- zeroValue\n",
    "\n",
    "[1, 2]                  [3, 4]\n",
    "\n",
    "0 + 1 = 1               0 + 3 = 3\n",
    "0 + 1 = 1               0 + 1 = 1\n",
    "\n",
    "1 + 2 = 3               3 + 4 = 7\n",
    "1 + 1 = 2               1 + 1 = 2       \n",
    "    |                       |\n",
    "    v                       v\n",
    "  (3, 2)                  (7, 2)\n",
    "      \\                    / \n",
    "       \\                  /\n",
    "        \\                /\n",
    "         \\              /\n",
    "          \\            /\n",
    "           \\          / \n",
    "           ------------\n",
    "           |  combOp  |\n",
    "           ------------\n",
    "                |\n",
    "                v\n",
    "             (10, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4]\n",
      "(10, 24)\n"
     ]
    }
   ],
   "source": [
    "# aggregate Addition and Multiplication\n",
    "x = sc.parallelize([1,2,3,4],2)\n",
    "neutral_zero_value = (0,1) # sum: x+0 = x, product: 1*x = x\n",
    "seqOp = (lambda aggregated, el: (aggregated[0] + el, aggregated[1] * el)) \n",
    "combOp = (lambda aggregated, el: (aggregated[0] + el[0], aggregated[1] * el[1]))\n",
    "y = x.aggregate(neutral_zero_value,seqOp,combOp)  # computes (cumulative sum, cumulative product)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "          (0, 0) <-- zeroValue\n",
    "\n",
    "[1, 2]                  [3, 4]\n",
    "\n",
    "0 + 1 = 1               0 + 3 = 3\n",
    "1 * 1 = 1               1 * 3 = 3\n",
    "\n",
    "1 + 2 = 3               3 + 4 = 7\n",
    "1 * 2 = 2               3 * 4 = 12       \n",
    "    |                       |\n",
    "    v                       v\n",
    "  (3, 2)                  (7, 12)\n",
    "      \\                    / \n",
    "       \\                  /\n",
    "        \\                /\n",
    "         \\              /\n",
    "          \\            /\n",
    "           \\          / \n",
    "           ------------\n",
    "           |  combOp  | (3+7,2*12)\n",
    "           ------------\n",
    "                |\n",
    "                v\n",
    "             (10, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.max\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page26.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Find the maximum item in this RDD.\n",
       "\n",
       ":param key: A function used to generate key for comparing\n",
       "\n",
       ">>> rdd = sc.parallelize([1.0, 5.0, 43.0, 10.0])\n",
       ">>> rdd.max()\n",
       "43.0\n",
       ">>> rdd.max(key=str)\n",
       "5.0\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.max?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 5.0, 10.0]\n",
      "10.0\n",
      "5.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# max\n",
    "x = sc.parallelize([1.0,5.0,10.0])\n",
    "y1=x.max() #Actual max value\n",
    "y2=x.max(key=str) #convert to string and find largest string (5.0>10.0)\n",
    "y = x.max(key=lambda x:-x) #-1.0 greater than all - Use max ax min\n",
    "print(x.collect())\n",
    "print(y1)\n",
    "print(y2)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.min\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page27.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Find the minimum item in this RDD.\n",
       "\n",
       ":param key: A function used to generate key for comparing\n",
       "\n",
       ">>> rdd = sc.parallelize([2.0, 5.0, 43.0, 10.0])\n",
       ">>> rdd.min()\n",
       "2.0\n",
       ">>> rdd.min(key=str)\n",
       "10.0\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.min?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD>min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# min\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.min(key=lambda x:-x)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sum\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page28.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Add up the elements in this RDD.\n",
       "\n",
       ">>> sc.parallelize([1.0, 2.0, 3.0]).sum()\n",
       "6.0\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.sum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3], [2, 4], [5, 6]]\n",
      "21\n"
     ]
    }
   ],
   "source": [
    "# sum\n",
    "x = sc.parallelize([1,3,2,4,5,6],3)\n",
    "y=x.sum()\n",
    "print(x.glom().collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.count\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page29.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return the number of elements in this RDD.\n",
       "\n",
       ">>> sc.parallelize([2, 3, 4]).count()\n",
       "3\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.count?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 2]\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "x = sc.parallelize([1,3,2],3)\n",
    "y = x.count()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.histogram\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page30.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Compute a histogram using the provided buckets. The buckets\n",
       "are all open to the right except for the last which is closed.\n",
       "e.g. [1,10,20,50] means the buckets are [1,10) [10,20) [20,50],\n",
       "which means 1<=x<10, 10<=x<20, 20<=x<=50. And on the input of 1\n",
       "and 50 we would have a histogram of 1,0,1.\n",
       "\n",
       "If your histogram is evenly spaced (e.g. [0, 10, 20, 30]),\n",
       "this can be switched from an O(log n) inseration to O(1) per\n",
       "element (where n is the number of buckets).\n",
       "\n",
       "Buckets must be sorted, not contain any duplicates, and have\n",
       "at least two elements.\n",
       "\n",
       "If `buckets` is a number, it will generate buckets which are\n",
       "evenly spaced between the minimum and maximum of the RDD. For\n",
       "example, if the min value is 0 and the max is 100, given `buckets`\n",
       "as 2, the resulting buckets will be [0,50) [50,100]. `buckets` must\n",
       "be at least 1. An exception is raised if the RDD contains infinity.\n",
       "If the elements in the RDD do not vary (max == min), a single bucket\n",
       "will be used.\n",
       "\n",
       "The return value is a tuple of buckets and histogram.\n",
       "\n",
       ">>> rdd = sc.parallelize(range(51))\n",
       ">>> rdd.histogram(2)\n",
       "([0, 25, 50], [25, 26])\n",
       ">>> rdd.histogram([0, 5, 25, 50])\n",
       "([0, 5, 25, 50], [5, 20, 26])\n",
       ">>> rdd.histogram([0, 15, 30, 45, 60])  # evenly spaced buckets\n",
       "([0, 15, 30, 45, 60], [15, 15, 15, 6])\n",
       ">>> rdd = sc.parallelize([\"ab\", \"ac\", \"b\", \"bd\", \"ef\"])\n",
       ">>> rdd.histogram((\"a\", \"b\", \"c\"))\n",
       "(('a', 'b', 'c'), [2, 2])\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/spark/python/pyspark/rdd.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RDD.histogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1, 2, 3]\n",
      "([1, 2, 3], [2, 3])\n"
     ]
    }
   ],
   "source": [
    "# histogram (example #1)\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.histogram(buckets = 2)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 1, 2, 3]\n",
      "([0, 0.5, 1, 1.5, 2, 2.5, 3, 3.5], [0, 0, 2, 0, 1, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "# histogram (example #2)\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.histogram([0,0.5,1,1.5,2,2.5,3,3.5])\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mean\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page31.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.mean()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.variance\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page32.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.variance()  # divides by N\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.stdev\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page33.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdev\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.stdev()  # divides by N\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sampleStdev\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page34.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampleStdev\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.sampleStdev() # divides by N-1\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sampleVariance\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page35.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampleVariance\n",
    "x = sc.parallelize([1,3,2])\n",
    "y = x.sampleVariance()  # divides by N-1\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.countByValue\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page36.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countByValue\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.countByValue()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.top\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page37.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.top(num = 3)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.takeOrdered\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page38.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takeOrdered\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.takeOrdered(num = 3)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.take\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page39.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.take(num = 3)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.first\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page40.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first\n",
    "x = sc.parallelize([1,3,1,2,3])\n",
    "y = x.first()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.collectAsMap\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page41.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collectAsMap\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
    "y = x.collectAsMap()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.keys\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page42.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
    "y = x.keys()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.values\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page43.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values\n",
    "x = sc.parallelize([('C',3),('A',1),('B',2)])\n",
    "y = x.values()\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduceByKey\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page44.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduceByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.reduceByKey(lambda agg, obj: agg + obj)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.reduceByKeyLocally\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page45.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduceByKeyLocally\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.reduceByKeyLocally(lambda agg, obj: agg + obj)\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.countByKey\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page46.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# countByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "y = x.countByKey()\n",
    "print(x.collect())\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.join\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page47.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
    "z = x.join(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.leftOuterJoin\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page48.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leftOuterJoin\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
    "z = x.leftOuterJoin(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.rightOuterJoin\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page49.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rightOuterJoin\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',5)])\n",
    "z = x.rightOuterJoin(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.partitionBy\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page50.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitionBy\n",
    "x = sc.parallelize([(0,1),(1,2),(2,3)],2)\n",
    "y = x.partitionBy(numPartitions = 3, partitionFunc = lambda x: x)  # only key is passed to paritionFunc\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.combineByKey\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page51.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combineByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "createCombiner = (lambda el: [(el,el**2)]) \n",
    "mergeVal = (lambda aggregated, el: aggregated + [(el,el**2)]) # append to aggregated\n",
    "mergeComb = (lambda agg1,agg2: agg1 + agg2 )  # append agg1 with agg2\n",
    "y = x.combineByKey(createCombiner,mergeVal,mergeComb)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.aggregateByKey\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page52.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregateByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "zeroValue = [] # empty list is 'zero value' for append operation\n",
    "mergeVal = (lambda aggregated, el: aggregated + [(el,el**2)])\n",
    "mergeComb = (lambda agg1,agg2: agg1 + agg2 )\n",
    "y = x.aggregateByKey(zeroValue,mergeVal,mergeComb)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.foldByKey\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page53.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# foldByKey\n",
    "x = sc.parallelize([('B',1),('B',2),('A',3),('A',4),('A',5)])\n",
    "zeroValue = 1 # one is 'zero value' for multiplication\n",
    "y = x.foldByKey(zeroValue,lambda agg,x: agg*x )  # computes cumulative product within each key\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupByKey\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page54.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupByKey\n",
    "x = sc.parallelize([('B',5),('B',4),('A',3),('A',2),('A',1)])\n",
    "y = x.groupByKey()\n",
    "print(x.collect())\n",
    "print([(j[0],[i for i in j[1]]) for j in y.collect()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.flatMapValues\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page55.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatMapValues\n",
    "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
    "y = x.flatMapValues(lambda x: [i**2 for i in x]) # function is applied to entire value, then result is flattened\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.mapValues\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page56.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapValues\n",
    "x = sc.parallelize([('A',(1,2,3)),('B',(4,5))])\n",
    "y = x.mapValues(lambda x: [i**2 for i in x]) # function is applied to entire value\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.groupWith\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page57.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# groupWith\n",
    "x = sc.parallelize([('C',4),('B',(3,3)),('A',2),('A',(1,1))])\n",
    "y = sc.parallelize([('B',(7,7)),('A',6),('D',(5,5))])\n",
    "z = sc.parallelize([('D',9),('B',(8,8))])\n",
    "a = x.groupWith(y,z)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())\n",
    "print(\"Result:\")\n",
    "for key,val in list(a.collect()): \n",
    "    print(key, [list(i) for i in val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.cogroup\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page58.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cogroup\n",
    "x = sc.parallelize([('C',4),('B',(3,3)),('A',2),('A',(1,1))])\n",
    "y = sc.parallelize([('A',8),('B',7),('A',6),('D',(5,5))])\n",
    "z = x.cogroup(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "for key,val in list(z.collect()):\n",
    "    print(key, [list(i) for i in val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.sampleByKey\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page59.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampleByKey\n",
    "x = sc.parallelize([('A',1),('B',2),('C',3),('B',4),('A',5)])\n",
    "y = x.sampleByKey(withReplacement=False, fractions={'A':0.5, 'B':1, 'C':0.2})\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.subtractByKey\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page60.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtractByKey\n",
    "x = sc.parallelize([('C',1),('B',2),('A',3),('A',4)])\n",
    "y = sc.parallelize([('A',5),('D',6),('A',7),('D',8)])\n",
    "z = x.subtractByKey(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.subtract\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page61.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtract\n",
    "x = sc.parallelize([('C',4),('B',3),('A',2),('A',1)])\n",
    "y = sc.parallelize([('C',8),('A',2),('D',1)])\n",
    "z = x.subtract(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.keyBy\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page62.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keyBy\n",
    "x = sc.parallelize([1,2,3])\n",
    "y = x.keyBy(lambda x: x**2)\n",
    "print(x.collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.repartition\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page63.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repartition\n",
    "x = sc.parallelize([1,2,3,4,5],2)\n",
    "y = x.repartition(numPartitions=3)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.coalesce\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page64.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coalesce\n",
    "x = sc.parallelize([1,2,3,4,5],2)\n",
    "y = x.coalesce(numPartitions=1)\n",
    "print(x.glom().collect())\n",
    "print(y.glom().collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.zip\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page65.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip\n",
    "x = sc.parallelize(['B','A','A'])\n",
    "y = x.map(lambda x: ord(x))  # zip expects x and y to have same #partitions and #elements/partition\n",
    "z = x.zip(y)\n",
    "print(x.collect())\n",
    "print(y.collect())\n",
    "print(z.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.zipWithIndex\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page66.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipWithIndex\n",
    "x = sc.parallelize(['B','A','A'],2)\n",
    "y = x.zipWithIndex()\n",
    "print(x.glom().collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"http://spark.apache.org/docs/1.2.0/api/python/pyspark.html#pyspark.RDD.zipWithUniqueId\">\n",
    "<img align=left src=\"Share/files/images/pyspark-page67.svg\" width=500 height=500 />\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zipWithUniqueId\n",
    "x = sc.parallelize(['B','A','A'],2)\n",
    "y = x.zipWithUniqueId()\n",
    "print(x.glom().collect())\n",
    "print(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 2",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
