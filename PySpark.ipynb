{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pySPARK - BASIC KNOWLEDGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This book contains data of notebook pySPARK in SparkPractices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This book follows the progress the author has made on the topic nad will contain different versions of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Local Machine to find Spark Home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'C:\\\\pyspark'"
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using OLD style SparkContext creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SparkContext, SparkConf\n",
    "from pyspark import SparkConf,SparkContext\n",
    "conf=SparkConf().setMaster('local').setAppName('Testing')\n",
    "sc=SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NEW style SparkSession creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark=SparkSession. \\\n",
    "builder. \\\n",
    "appName('PySpark'). \\\n",
    "master('local'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-249MU1B.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x6b52c88>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc=spark.sparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READING DATA FROM A FILE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### On VAGRANT VM\n",
    "##### \"--master yarn\" can only read from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderItems=sc.textFile('/public/retail_db/order_items')\n",
    "\n",
    "#To read from local, we need \"--master local[*]\"\n",
    "blog=sc.textFile('file:///home/vagrant/SparkPractices/Share/blogtexts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FOR LOCAL\n",
    "##### Can read from HDFS and local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orderItems=sc.textFile('D://Bigdata Tutorials//data//retail_db//order_items//part-00000')\n",
    "\n",
    "blog=sc.textFile('Share//blogtexts')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's work with BlogTexts example of wordcount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1  - Lets find some rows of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Think of it for a moment \\u2013 1 Qunitillion = 1 Million Billion! Can you imagine how many drives / CDs / Blue-ray DVDs would be required to store them? It is difficult to imagine this scale of data generation even as a data science professional. While this pace of data generation is very exciting,  it has created entirely new set of challenges and has forced us to find new ways to handle Big Huge data effectively.',\n",
       " u'',\n",
       " u'Big Data is not a new phenomena. It has been around for a while now. However, it has become really important with this pace of data generation. In past, several systems were developed for processing big data. Most of them were based on MapReduce framework. These frameworks typically rely on use of hard disk for saving and retrieving the results. However, this turns out to be very costly in terms of time and speed.',\n",
       " u'',\n",
       " u'On the other hand, Organizations have never been more hungrier to add a competitive differentiation through understanding this data and offering its customer a much better experience. Imagine how valuable would be Facebook, if it did not understand your interests well? The traditional hard disk based MapReduce kind of frameworks do not help much to address this challenge.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blog.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's tackle transformations first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map and flatMap\n",
    "\n",
    "### Use Case\n",
    "\n",
    "#### Suppose we want to convert all letters in RDD to lowercase and split the lines using \" \"(space)\n",
    "  \n",
    "#### map(): To lower the case of each word of a document & split the line, we can use the map transformation.\n",
    "  * Do note that the output is not FLAT, i.e. it has nested list.\n",
    "  \n",
    "#### flatMap: Same as map, but as an extra step, it flattens the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogMap=blog.map(lambda x: x.lower().split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'think', u'of', u'it', u'for', u'a', u'moment', u'\\u2013', u'1', u'qunitillion', u'=', u'1', u'million', u'billion!', u'can', u'you', u'imagine', u'how', u'many', u'drives', u'/', u'cds', u'/', u'blue-ray', u'dvds', u'would', u'be', u'required', u'to', u'store', u'them?', u'it', u'is', u'difficult', u'to', u'imagine', u'this', u'scale', u'of', u'data', u'generation', u'even', u'as', u'a', u'data', u'science', u'professional.', u'while', u'this', u'pace', u'of', u'data', u'generation', u'is', u'very', u'exciting,', u'', u'it', u'has', u'created', u'entirely', u'new', u'set', u'of', u'challenges', u'and', u'has', u'forced', u'us', u'to', u'find', u'new', u'ways', u'to', u'handle', u'big', u'huge', u'data', u'effectively.']]\n"
     ]
    }
   ],
   "source": [
    "# This gives O/P as list of lists which is too long to show using first() or take()\n",
    "print(blogMap.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogFlatMap=blog.flatMap(lambda x: x.lower().split(\" \"),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'think', u'of', u'it', u'for', u'a', u'moment', u'\\u2013', u'1', u'qunitillion', u'=', u'1', u'million', u'billion!', u'can', u'you', u'imagine', u'how', u'many', u'drives', u'/']\n"
     ]
    }
   ],
   "source": [
    "print(blogFlatMap.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE - To make map same as flatMap, we can use collect().\n",
    "#### Thus using collect to convert it from RDD to a list and then using indexes to show only 5 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'think', u'of', u'it', u'for', u'a', u'moment', u'\\u2013', u'1', u'qunitillion', u'=', u'1', u'million', u'billion!', u'can', u'you', u'imagine', u'how', u'many', u'drives', u'/']\n"
     ]
    }
   ],
   "source": [
    "print(blogMap.collect()[0][:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now we have a collection of words and symbols on different indexes of a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FILTER out words that are not needed\n",
    "\n",
    "#### We call these words as “stop words”; Stop words do not add much value in a text. For example, “is”, “am”, “are” and “the” are few examples of stop words.\n",
    "\n",
    "* Let's try to filter these words out:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "blogFiltered=blogFlatMap.filter(lambda x: x not in ['is','am','are','the','a','for','/','1','='])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'think', u'of', u'it', u'moment', u'\\u2013', u'qunitillion', u'million', u'billion!', u'can', u'you', u'imagine', u'how', u'many', u'drives', u'cds', u'blue-ray', u'dvds', u'would', u'be', u'required']\n"
     ]
    }
   ],
   "source": [
    "print(blogFiltered.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note, if this was a wordcount example, we would just need to do the following:\n",
    "* Use map to create (key,value) pairs where starting value will be 1.\n",
    "* We will use reduceByKey() to get the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'think', 1), (u'of', 1), (u'it', 1), (u'moment', 1), (u'\\u2013', 1), (u'qunitillion', 1), (u'million', 1), (u'billion!', 1), (u'can', 1), (u'you', 1), (u'imagine', 1), (u'how', 1), (u'many', 1), (u'drives', 1), (u'cds', 1), (u'blue-ray', 1), (u'dvds', 1), (u'would', 1), (u'be', 1), (u'required', 1)]\n"
     ]
    }
   ],
   "source": [
    "from operator import add\n",
    "\n",
    "blogTuples=blogFiltered.map(lambda x: (x,1))\n",
    "print(blogTuples.take(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now to find and print the WordCount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(274, u''), (164, u'to'), (143, u'in'), (122, u'of'), (106, u'and'), (103, u'we'), (69, u'spark'), (64, u'this'), (63, u'data'), (55, u'can'), (52, u'apache'), (40, u'it'), (40, u'on'), (39, u'which'), (32, u'with'), (32, u'will'), (31, u'you'), (31, u'by'), (30, u'rdd'), (28, u'as')]\n"
     ]
    }
   ],
   "source": [
    "blogResult=blogTuples.reduceByKey(add).map(lambda x: (x[1],x[0])).sortByKey(False)\n",
    "print(blogResult.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{u'': 274, u'saves': 2, u'elements,': 2, u'step1:': 1, u'cluster).': 1, u'pyspark_driver_python=ipython': 1, u'skills': 1, u'better.': 1, u'connects': 1, u\"('am',\": 1, u'solution': 3, u'sc.parallelize(data)': 2, u'elegant': 1, u'diff_cat_in_train_test.distinct().count()#': 1, u'sc.parallelize(data,': 1, u'otherwise,': 1, u'saved': 1, u'(which': 1, u'created.': 1, u'second': 1}\n"
     ]
    }
   ],
   "source": [
    "#Another way to find the word count:\n",
    "\n",
    "blogCount=blogTuples.countByKey()\n",
    "\n",
    "# This returns a dictionary, which we can edit in a such a way to show only some data\n",
    "##This is done by explicitly converting it into a list, splicing the list and then reverting back to a dict.\n",
    "\n",
    "print(dict(list(blogCount.items())[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instead of these, we can also try groupByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'', [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], [u'diff_cat_in_train_test.distinct().count()#', [1]], [u'better.', [1]], [u'elements,', [1, 1]], [u'saved', [1]]]\n"
     ]
    }
   ],
   "source": [
    "blogGroup=blogTuples.groupByKey()\n",
    "print(list([j[0],list(j[1])] for j in blogGroup.take(5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(274, u''), (164, u'to'), (143, u'in'), (122, u'of'), (106, u'and'), (103, u'we'), (69, u'spark'), (64, u'this'), (63, u'data'), (55, u'can'), (52, u'apache'), (40, u'it'), (40, u'on'), (39, u'which'), (32, u'with'), (32, u'will'), (31, u'you'), (31, u'by'), (30, u'rdd'), (28, u'as')]\n"
     ]
    }
   ],
   "source": [
    "#sortByKey() -- True - ASC and FALSE - DESC\n",
    "blogCount2=blogGroup.mapValues(sum).map(lambda x: (x[1],x[0])).sortByKey(False)\n",
    "print(blogCount2.take(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference B/W reduceByKey and groupByKey\n",
    "\n",
    "* The “reduceByKey” transformations first combined the values for each key in all partition, so each partition will have only one value for a key then after shuffling, in reduce phase executors will apply operation for example, in my case sum(lambda x: x+y).\n",
    "\n",
    "<div><img align=left src=\"Share/files/images/reduceByKey-3.png\" width=500 height=500 /><img align=left src=\"Share/files/images/groupbykey.png\" width=500 height=500 /></div>\n",
    "\n",
    "* But in case of “groupByKey” transformation, it will not combine the values in each key in all partition it directly shuffle the data then merge the values for each key. Here in “groupByKey” transformation lot of shuffling in the data is required to get the answer, so it is better to use “reduceByKey” in case of large shuffling of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This completes the WORDCOUNT example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But, we can do much more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## groupBy\n",
    "\n",
    "### Step 3: Now, we want to group the words in blogFiltered based on which letters they start with.\n",
    "\n",
    "* This means that, Key will be a substring and Value will be all values that start with those values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    if x.startswith('all'):\n",
    "        return x\n",
    "blogGroupIt=blogFiltered.groupBy(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'all',\n",
       "  [u'all',\n",
       "   u'all',\n",
       "   u'all',\n",
       "   u'all',\n",
       "   u'all',\n",
       "   u'all',\n",
       "   u'all',\n",
       "   u'all',\n",
       "   u'all',\n",
       "   u'all']),\n",
       " (u'allows', [u'allows'])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(k, list(v)) for (k, v) in blogGroupIt.take(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mapPartition\n",
    "\n",
    "### STEP 4\n",
    "* We can use it to count the words ‘spark’ and ‘apache’ in blogFiltered, separatly, on each partition and get the output of the task performed in these partition\n",
    "\n",
    "* We can do this by applying “mapPartitions” transformation. The “mapPartitions” is like a map transformation but runs separately on different partitions of a RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 23], [39, 29]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func(iterator):\n",
    "  count_spark = 0\n",
    "  count_apache = 0\n",
    "  for i in iterator:\n",
    "     if i =='spark':\n",
    "        count_spark = count_spark + 1\n",
    "     if i == 'apache':\n",
    "        count_apache = count_apache + 1\n",
    "  return (count_spark,count_apache)\n",
    "\n",
    "blogFiltered.repartition(2).mapPartitions(func,True).glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation: sample\n",
    "### STEP 5 \n",
    "\n",
    "* What if I want to work with samples instead of full data ?\n",
    "\n",
    "* “sample” transformation helps us in taking samples instead of working on full data. \n",
    "\n",
    "* The sample method will return a new RDD, containing a statistical sample of the original RDD.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mSignature:\u001b[0m \u001b[0mblog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwithReplacement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfraction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m\n",
       "Return a sampled subset of this RDD.\n",
       "\n",
       ":param withReplacement: can elements be sampled multiple times (replaced when sampled out)\n",
       ":param fraction: expected size of the sample as a fraction of this RDD's size\n",
       "    without replacement: probability that each element is chosen; fraction must be [0, 1]\n",
       "    with replacement: expected number of times each element is chosen; fraction must be >= 0\n",
       ":param seed: seed for the random number generator\n",
       "\n",
       ".. note:: This is not guaranteed to provide exactly the fraction specified of the total\n",
       "    count of the given :class:`DataFrame`.\n",
       "\n",
       ">>> rdd = sc.parallelize(range(100), 4)\n",
       ">>> 6 <= rdd.sample(False, 0.1, 81).count() <= 14\n",
       "True\n",
       "\u001b[1;31mFile:\u001b[0m      c:\\pyspark\\python\\pyspark\\rdd.py\n",
       "\u001b[1;31mType:\u001b[0m      instancemethod\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "blog.sample?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction = 0.3 means 30% of total dataset\n",
    "#seed = for a random number generation to be always ben same\n",
    "blogFilteredSample=blogFiltered.sample(False,0.3,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4989, 1509)\n"
     ]
    }
   ],
   "source": [
    "print(len(blogFiltered.collect()),len(blogFilteredSample.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformation: Union\n",
    "### STEP 6\n",
    "\n",
    "* What if I want to work with multiple samples?\n",
    "\n",
    "\n",
    "* “union” transformation will return a new RDD by taking the union of two RDDs. \n",
    "\n",
    "* Note that duplicate items will not be removed in the new RDD.\n",
    "\n",
    "* To show the same:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same sample copy\n",
    "blogFilteredSample2=blogFilteredSample\n",
    "\n",
    "#OR \n",
    "#blogFilteredSample2=blogFiltered.sample(False,0.3,30)\n",
    "\n",
    "#Union of the two samples:\n",
    "blogFilteredSampleUnion=blogFilteredSample.union(blogFilteredSample2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1509, 1509, 3018)\n"
     ]
    }
   ],
   "source": [
    "print(len(blogFilteredSample.collect()),len(blogFilteredSample2.collect()),len(blogFilteredSampleUnion))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thus, we can see that DUPLICATES were not removed!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}